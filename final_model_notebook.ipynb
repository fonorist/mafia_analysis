{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa6397b",
   "metadata": {},
   "source": [
    "# 🎓 Mafia Game — Pre-Game Winner Prediction\n",
    "\n",
    "This notebook builds a **leak-free, pre-game prediction model** for Mafia games.\n",
    "We go from **clean per-player rows** to **per-game team probabilities** using:\n",
    "- Temporal **Elo with decay** (skills evolve over time)\n",
    "- **Role & side** rolling performance\n",
    "- **Breaks**/freshness\n",
    "- **Role-specific history** (experience & win rates on each role)\n",
    "- **Synergy** (same-team familiarity) & **Enemy familiarity** (cross-team history)\n",
    "- **Streaks** (win/loss momentum)\n",
    "- **Meta-eras** (ruleset changes over time)\n",
    "- **Team aggregation** → opponent deltas\n",
    "- **LightGBM** (main) and optional **CatBoost** (comparison), with **probability calibration**\n",
    "- Proper **time-aware evaluation** (holdout = last 15% by time proxy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66121bb5",
   "metadata": {},
   "source": [
    "## 1) Environment & Imports\n",
    "\n",
    "If something is missing, install via the first cell. Then import everything we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bb3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, uncomment to install packages\n",
    "# !pip install -q lightgbm catboost scikit-learn optuna matplotlib pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Modeling & metrics\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Optional\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6a096",
   "metadata": {},
   "source": [
    "## 2) Configuration & Data Load\n",
    "\n",
    "- **Input:** a cleaned per-player table (one row per player per game).  \n",
    "  Columns required (min): `id, game_id, player_id, role, team, place, game_points, team_win`  \n",
    "- **Assumptions:** each game has **10 players**; exactly **one team wins** (7 winners if citizens, 3 if mafia).  \n",
    "- **Time proxy:** `id` increases with time.\n",
    "\n",
    "> Update `DATA_CSV` if your file is in a different location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97a8a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (802820, 21) columns: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_44808\\712523340.py:4: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_CSV)\n"
     ]
    }
   ],
   "source": [
    "DATA_CSV = Path(\"cleaned/mafia_clean.csv\")   # put the CSV next to this notebook or provide an absolute path\n",
    "OUT_DIR  = Path(\"cleaned\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print(\"Loaded:\", df.shape, \"columns:\", len(df.columns))\n",
    "assert {'id','game_id','player_id','role','team','game_points','team_win'}.issubset(df.columns), \\\n",
    "    \"Missing required columns in the cleaned dataset.\"\n",
    "\n",
    "# Basic coercions\n",
    "df['id'] = pd.to_numeric(df['id'], errors='coerce').astype('int64')\n",
    "df['game_id'] = pd.to_numeric(df['game_id'], errors='coerce').astype('int64')\n",
    "df['player_id'] = pd.to_numeric(df['player_id'], errors='coerce').astype('int64')\n",
    "df['team_win'] = pd.to_numeric(df['team_win'], errors='coerce').astype('int8')\n",
    "df['team'] = df['team'].astype('category')\n",
    "df['role'] = df['role'].astype('category')\n",
    "\n",
    "# Seat/position optional column name normalization (if present)\n",
    "if 'place' in df.columns:\n",
    "    df['place'] = pd.to_numeric(df['place'], errors='coerce').fillna(0).astype('int16')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec477f",
   "metadata": {},
   "source": [
    "## 3) Helper Utilities\n",
    "\n",
    "Small helpers for quantiles and sanity checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79594864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q25(x): return np.nanpercentile(x, 25)\n",
    "def q75(x): return np.nanpercentile(x, 75)\n",
    "\n",
    "def sanity_assert_two_rows_per_game(team_tall):\n",
    "    cnt = team_tall.groupby('game_id').size().value_counts()\n",
    "    print(\"Rows per game distribution:\\n\", cnt.head())\n",
    "    assert 2 in cnt.index.tolist(), \"Every game should have exactly 2 rows (one per team).\"\n",
    "\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    inds = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        mask = inds == b\n",
    "        if mask.sum() == 0: \n",
    "            continue\n",
    "        conf = y_prob[mask].mean()\n",
    "        acc  = y_true[mask].mean()\n",
    "        ece += (mask.mean()) * abs(acc - conf)\n",
    "    return ece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92920e65",
   "metadata": {},
   "source": [
    "## 4) Feature Engineering (Player-level)\n",
    "\n",
    "We compute **pre-game** features only (no leakage):\n",
    "- **Temporal Elo with decay** (global, by side, by role)\n",
    "- **Side & role** rolling performance\n",
    "- **Breaks/freshness** via id gaps\n",
    "- **Role-specific history** (experience and WR on that role)\n",
    "- **Same-team synergy** & **Enemy familiarity**\n",
    "- **Streaks** (win/loss)\n",
    "- **Meta eras** (bucket by id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a43a6",
   "metadata": {},
   "source": [
    "### 4.1 Meta eras & gap features\n",
    "- `meta_period`: bucket `id` into eras to capture rule changes.\n",
    "- `gap_id` per player → `gap_id_clipped` (bounded) and `long_break_flag`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ff9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta eras\n",
    "bins   = [0, 200_000, 400_000, 600_000, 800_000, 1_000_000_000]\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "df['meta_period'] = pd.cut(df['id'], bins=bins, labels=labels, include_lowest=True).astype('int8')\n",
    "\n",
    "# Gap per player (id as time proxy)\n",
    "df = df.sort_values(['player_id','id']).copy()\n",
    "df['gap_id'] = df.groupby('player_id')['id'].diff().fillna(0).astype('int64')\n",
    "df['gap_id_clipped'] = np.clip(df['gap_id'], 0, 5000).astype('int32')\n",
    "GAP_THRESH = 381  # adjust via quantiles if desired\n",
    "df['long_break_flag'] = (df['gap_id'] >= GAP_THRESH).astype('int8')\n",
    "\n",
    "# Restore global order\n",
    "df = df.sort_values('id').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62bd0e",
   "metadata": {},
   "source": [
    "### 4.2 Temporal Elo with decay\n",
    "We update Elo **after** each game. Each update is scaled by `exp(-gap/tau)` so **older inactivity** reduces update size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2843edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elos(dfin, init=1500, k=24, tau=300.0):\n",
    "    d = dfin.sort_values('id').copy()\n",
    "    elo_global, elo_side, elo_role = {}, {}, {}\n",
    "    last_seen = {}\n",
    "    outs = []\n",
    "\n",
    "    for gid, g in d.groupby('game_id', sort=False):\n",
    "        cur = g.copy()\n",
    "        cur['pre_elo']      = [elo_global.get(pid, init) for pid in cur['player_id']]\n",
    "        cur['pre_elo_side'] = [elo_side.get((pid, team), init) for pid, team in zip(cur['player_id'], cur['team'])]\n",
    "        cur['pre_elo_role'] = [elo_role.get((pid, role), init) for pid, role in zip(cur['player_id'], cur['role'])]\n",
    "\n",
    "        maf_mask  = cur['team'].eq('mafia')\n",
    "        mafia_mu  = cur.loc[maf_mask, 'pre_elo'].mean()\n",
    "        citizen_mu= cur.loc[~maf_mask, 'pre_elo'].mean()\n",
    "        exp_mafia = 1.0 / (1.0 + 10 ** ((citizen_mu - mafia_mu)/400))\n",
    "        mafia_res = int(cur.loc[maf_mask, 'team_win'].iloc[0])\n",
    "\n",
    "        for _, r in cur.iterrows():\n",
    "            pid, side, role, rid = int(r['player_id']), r['team'], r['role'], int(r['id'])\n",
    "            gap = rid - last_seen.get(pid, rid)\n",
    "            decay = float(np.exp(-max(gap,0)/float(tau)))\n",
    "            exp = exp_mafia if side=='mafia' else (1-exp_mafia)\n",
    "            act = mafia_res if side=='mafia' else (1-mafia_res)\n",
    "            delta = k * decay * (act - exp)\n",
    "\n",
    "            elo_global[pid] = elo_global.get(pid,  init) + delta\n",
    "            elo_side[(pid, side)] = elo_side.get((pid, side), init) + delta\n",
    "            elo_role[(pid, role)] = elo_role.get((pid, role), init) + delta\n",
    "            last_seen[pid] = rid\n",
    "\n",
    "        outs.append(cur[['game_id','player_id','pre_elo','pre_elo_side','pre_elo_role']])\n",
    "\n",
    "    elo_df = pd.concat(outs, ignore_index=True)\n",
    "    return d.merge(elo_df, on=['game_id','player_id'], how='left')\n",
    "\n",
    "work_players = compute_elos(df, init=1500, k=24, tau=300.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cf85f",
   "metadata": {},
   "source": [
    "### 4.3 Side-aware rolling win rates\n",
    "We track recent **team win** rates for each player **on each side** separately (mafia/citizens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "114831a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_stats_side(df, windows=(5,20)):\n",
    "    d = df.sort_values(['player_id','id']).copy()\n",
    "    for side in ['mafia','citizens']:\n",
    "        mask = d['team'].eq(side)\n",
    "        d.loc[mask, f'roll5_win_rate_{side}']  = d.loc[mask].groupby('player_id')['team_win'].shift(1).rolling(windows[0], min_periods=1).mean().values\n",
    "        d.loc[mask, f'roll20_win_rate_{side}'] = d.loc[mask].groupby('player_id')['team_win'].shift(1).rolling(windows[1], min_periods=1).mean().values\n",
    "        d.loc[~mask, f'roll5_win_rate_{side}']  = 0.0\n",
    "        d.loc[~mask, f'roll20_win_rate_{side}'] = 0.0\n",
    "    return d\n",
    "\n",
    "work_players = add_rolling_stats_side(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27f27b",
   "metadata": {},
   "source": [
    "### 4.4 Role-specific history\n",
    "For each `(player, role)` compute:\n",
    "- `games_in_role` (prior count)\n",
    "- `win_rate_role_<role>_last{W}` for W in {5, 20, 50}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ebe186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_44808\\1064261818.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for (pid, role), g in d.groupby(['player_id','role'], sort=False):\n"
     ]
    }
   ],
   "source": [
    "def add_role_history_stats(df, windows=(5,20,50)):\n",
    "    d = df.sort_values(['player_id','role','id']).copy()\n",
    "    out = []\n",
    "    for (pid, role), g in d.groupby(['player_id','role'], sort=False):\n",
    "        g = g.copy()\n",
    "        past = g['team_win'].shift(1)\n",
    "        g['games_in_role'] = np.arange(len(g), dtype=np.int32)\n",
    "        for w in windows:\n",
    "            g[f'win_rate_role_{role}_last{w}'] = past.rolling(w, min_periods=1).mean()\n",
    "        out.append(g)\n",
    "    return pd.concat(out, ignore_index=True).sort_values('id').reset_index(drop=True)\n",
    "\n",
    "work_players = add_role_history_stats(work_players, windows=(5,20,50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22db51",
   "metadata": {},
   "source": [
    "### 4.5 Same-team synergy\n",
    "Count prior **same-team co-plays** for all teammate pairs before this game, aggregate per team.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5dbfd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def add_synergy_features(df):\n",
    "    d = df.copy()\n",
    "    game_order = (d.groupby('game_id')['id'].max().sort_values().index.tolist())\n",
    "    pair_counts = {}\n",
    "    out_rows = []\n",
    "\n",
    "    for gid in game_order:\n",
    "        g = d[d['game_id'] == gid]\n",
    "        for team in ['mafia', 'citizens']:\n",
    "            players = g.loc[g['team']==team, 'player_id'].dropna().astype(int).tolist()\n",
    "            vals = [pair_counts.get((a,b,team), 0) for a,b in combinations(sorted(players), 2)] if len(players)>=2 else []\n",
    "            s_mean = float(np.mean(vals)) if vals else 0.0\n",
    "            s_max  = float(np.max(vals))  if vals else 0.0\n",
    "            out_rows.append((gid, team, s_mean, s_max))\n",
    "        # update after\n",
    "        for team in ['mafia', 'citizens']:\n",
    "            players = g.loc[g['team']==team, 'player_id'].dropna().astype(int).tolist()\n",
    "            if len(players)>=2:\n",
    "                for a,b in combinations(sorted(players), 2):\n",
    "                    pair_counts[(a,b,team)] = pair_counts.get((a,b,team),0) + 1\n",
    "\n",
    "    team_synergy = pd.DataFrame(out_rows, columns=['game_id','team','synergy_mean_team','synergy_max_team'])\n",
    "    return d.merge(team_synergy, on=['game_id','team'], how='left')\n",
    "\n",
    "work_players = add_synergy_features(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0121f3",
   "metadata": {},
   "source": [
    "### 4.6 Enemy familiarity (cross-team history)\n",
    "Count how often each player has faced each opponent **before** this game. Aggregate per team.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd5bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def add_enemy_familiarity_features(df):\n",
    "    d = df.sort_values('id').copy()\n",
    "    game_order = (d.groupby('game_id')['id'].max().sort_values().index.tolist())\n",
    "    faced_counts = {}\n",
    "    out_rows = []\n",
    "\n",
    "    for gid in game_order:\n",
    "        g = d[d['game_id'] == gid]\n",
    "        maf = g[g['team']=='mafia']['player_id'].dropna().astype(int).tolist()\n",
    "        cit = g[g['team']=='citizens']['player_id'].dropna().astype(int).tolist()\n",
    "\n",
    "        pairs_maf = [faced_counts.get(tuple(sorted([a,b])), 0) for a,b in product(maf, cit)]\n",
    "        pairs_cit = [faced_counts.get(tuple(sorted([a,b])), 0) for a,b in product(cit, maf)]\n",
    "\n",
    "        def stats(vals):\n",
    "            return (float(np.mean(vals)) if vals else 0.0,\n",
    "                    float(np.max(vals))  if vals else 0.0)\n",
    "\n",
    "        maf_mean, maf_max = stats(pairs_maf)\n",
    "        cit_mean, cit_max = stats(pairs_cit)\n",
    "\n",
    "        out_rows.append((gid,'mafia',    maf_mean, maf_max))\n",
    "        out_rows.append((gid,'citizens', cit_mean, cit_max))\n",
    "\n",
    "        for a,b in product(maf, cit):\n",
    "            key = tuple(sorted([int(a),int(b)]))\n",
    "            faced_counts[key] = faced_counts.get(key, 0) + 1\n",
    "\n",
    "    fam = pd.DataFrame(out_rows, columns=['game_id','team','enemy_fam_mean_team','enemy_fam_max_team'])\n",
    "    return d.merge(fam, on=['game_id','team'], how='left')\n",
    "\n",
    "work_players = add_enemy_familiarity_features(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbf803",
   "metadata": {},
   "source": [
    "### 4.7 Win/Loss streaks\n",
    "Compute **pre-game** consecutive win and loss streak lengths for each player.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41e38950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_streak_features(df):\n",
    "    d = df.sort_values(['player_id','id']).copy()\n",
    "    win_streaks, loss_streaks = [], []\n",
    "\n",
    "    for pid, g in d.groupby('player_id', sort=False):\n",
    "        prev = g['team_win'].shift(1).values\n",
    "        w_stk = np.zeros(len(g), dtype=np.int16)\n",
    "        l_stk = np.zeros(len(g), dtype=np.int16)\n",
    "        cur_w = cur_l = 0\n",
    "        for i, v in enumerate(prev):\n",
    "            if np.isnan(v):\n",
    "                cur_w = cur_l = 0\n",
    "            else:\n",
    "                if v == 1:\n",
    "                    cur_w += 1; cur_l = 0\n",
    "                else:\n",
    "                    cur_l += 1; cur_w = 0\n",
    "            w_stk[i] = cur_w\n",
    "            l_stk[i] = cur_l\n",
    "        win_streaks.append(pd.Series(w_stk, index=g.index))\n",
    "        loss_streaks.append(pd.Series(l_stk, index=g.index))\n",
    "\n",
    "    d['win_streak']  = pd.concat(win_streaks).sort_index()\n",
    "    d['loss_streak'] = pd.concat(loss_streaks).sort_index()\n",
    "    return d.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "work_players = add_streak_features(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d7700",
   "metadata": {},
   "source": [
    "### 4.8 Games played to date (per player)\n",
    "Cumulative count of past games per player (pre-game). Useful as a general “experience” signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f835cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_games_played_feature(df):\n",
    "    d = df.sort_values(['player_id','id']).copy()\n",
    "    # number of *prior* appearances (shift to avoid leakage)\n",
    "    d['games_played'] = d.groupby('player_id').cumcount().astype('int32')\n",
    "    return d.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "work_players = add_games_played_feature(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190da785",
   "metadata": {},
   "source": [
    "## 5) Team-Level Aggregation & Deltas\n",
    "\n",
    "Aggregate player features to `(game_id, team)` rows. Then create **safe deltas**: `mafia − citizens`.  \n",
    "We **never** delta the target or `meta_period`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28abd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows per game distribution:\n",
      " 2    80282\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def build_team_agg(work_players, add_ratios=False, ratio_eps=1e-3):\n",
    "    agg_funcs = {}\n",
    "\n",
    "    def add_agg(col, funcs):\n",
    "        if col in work_players.columns:\n",
    "            agg_funcs[col] = funcs\n",
    "\n",
    "    # Core\n",
    "    add_agg('pre_elo', ['mean','std','min','max', q25, q75])\n",
    "    add_agg('pre_elo_side', ['mean'])\n",
    "    add_agg('pre_elo_role', ['mean'])\n",
    "    add_agg('gap_id_clipped', ['mean','max'])\n",
    "    add_agg('long_break_flag', ['sum'])\n",
    "    add_agg('place', ['mean','std','min','max'])\n",
    "    add_agg('games_played', ['mean','std','min','max'])  # if present\n",
    "\n",
    "    # Optional blocks\n",
    "    add_agg('win_streak', ['mean','max'])\n",
    "    add_agg('loss_streak', ['mean','max'])\n",
    "    add_agg('synergy_mean_team', ['mean'])\n",
    "    add_agg('synergy_max_team',  ['mean'])\n",
    "    add_agg('enemy_fam_mean_team', ['mean'])\n",
    "    add_agg('enemy_fam_max_team',  ['mean'])\n",
    "    add_agg('roll5_win_rate_mafia',  ['mean'])\n",
    "    add_agg('roll20_win_rate_mafia', ['mean'])\n",
    "    add_agg('roll5_win_rate_citizens',  ['mean'])\n",
    "    add_agg('roll20_win_rate_citizens', ['mean'])\n",
    "    if 'meta_period' in work_players.columns:\n",
    "        agg_funcs['meta_period'] = ['first']\n",
    "\n",
    "    base = work_players.groupby(['game_id','team']).agg(agg_funcs)\n",
    "    base.columns = ['_'.join([str(x) for x in c if x not in (None,)]).replace('<function ','').replace('>','')\n",
    "                    for c in base.columns]\n",
    "    base = base.reset_index()\n",
    "\n",
    "    # --- NEW: meta-period normalization for Elo stats (remove era drift) ---\n",
    "    if 'meta_period_first' in base.columns:\n",
    "        elo_cols = [c for c in base.columns if c.startswith('pre_elo_')]\n",
    "        for col in elo_cols:\n",
    "            # center within meta-period\n",
    "            base[f'{col}_norm'] = base[col] - base.groupby('meta_period_first')[col].transform('mean')\n",
    "\n",
    "    # Role-specific singletons/means\n",
    "    full_idx = base.set_index(['game_id','team']).index\n",
    "    # Role-specific singletons/means\n",
    "    full_idx = base.set_index(['game_id','team']).index\n",
    "\n",
    "    def single_role_stat(role, value_col, out_name):\n",
    "        s = (work_players[work_players['role']==role]\n",
    "             .groupby(['game_id','team'])[value_col].mean()).reindex(full_idx)\n",
    "        s.name = out_name; return s\n",
    "\n",
    "    def mean_role_stat(role, value_col, out_name):\n",
    "        s = (work_players[work_players['role']==role]\n",
    "             .groupby(['game_id','team'])[value_col].mean()).reindex(full_idx)\n",
    "        s.name = out_name; return s\n",
    "\n",
    "    pieces = [\n",
    "        single_role_stat('don','pre_elo_role','don_pre_elo_role'),\n",
    "        single_role_stat('sheriff','pre_elo_role','sheriff_pre_elo_role'),\n",
    "        single_role_stat('don','place','don_place'),\n",
    "        single_role_stat('sheriff','place','sheriff_place'),\n",
    "        mean_role_stat('black','pre_elo_role','black_mean_pre_elo_role'),\n",
    "        mean_role_stat('red','pre_elo_role','red_mean_pre_elo_role'),\n",
    "        single_role_stat('don','games_in_role','don_games_in_role'),\n",
    "        single_role_stat('sheriff','games_in_role','sheriff_games_in_role'),\n",
    "        mean_role_stat('black','games_in_role','black_mean_games_in_role'),\n",
    "        mean_role_stat('red','games_in_role','red_mean_games_in_role'),\n",
    "        single_role_stat('don','win_rate_role_don_last20','don_wr20'),\n",
    "        single_role_stat('sheriff','win_rate_role_sheriff_last20','sheriff_wr20'),\n",
    "        mean_role_stat('black','win_rate_role_black_last20','black_mean_wr20'),\n",
    "        mean_role_stat('red','win_rate_role_red_last20','red_mean_wr20'),\n",
    "    ]\n",
    "    role_feats = pd.concat(pieces, axis=1).reset_index()\n",
    "    team_agg = base.merge(role_feats, on=['game_id','team'], how='left')\n",
    "\n",
    "    # Label & time proxy\n",
    "    labels  = work_players.groupby(['game_id','team'])['team_win'].max().rename('team_win_team')\n",
    "    gmaxid  = work_players.groupby('game_id')['id'].max().rename('game_max_id')\n",
    "    team_agg = team_agg.merge(labels, on=['game_id','team']).merge(gmaxid, on='game_id')\n",
    "\n",
    "    # Safe deltas / ratios\n",
    "    wide = team_agg.pivot(index='game_id', columns='team')\n",
    "    wide.columns = [f\"{a}__{b}\" for a,b in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "\n",
    "    def side_cols(side): \n",
    "        return [c for c in wide.columns if c.endswith(f\"__{side}\") and c!='game_id']\n",
    "    maf_cols = side_cols('mafia')\n",
    "\n",
    "    delta = pd.DataFrame({'game_id': wide['game_id']})\n",
    "    skip_prefixes = ('team_win_team','meta_period')\n",
    "    for mcol in maf_cols:\n",
    "        base_name = mcol[:-len(\"__mafia\")]\n",
    "        if base_name.startswith(skip_prefixes): \n",
    "            continue\n",
    "        ccol = base_name + \"__citizens\"\n",
    "        if ccol in wide.columns:\n",
    "            delta[base_name + \"__delta_maf_minus_cit\"] = wide[mcol] - wide[ccol]\n",
    "            if add_ratios:\n",
    "                delta[base_name + \"__ratio_maf_over_cit\"] = (wide[mcol] + ratio_eps) / (wide[ccol] + ratio_eps)\n",
    "\n",
    "    team_tall = team_agg.merge(delta, on='game_id', how='left')\n",
    "\n",
    "    # --- NEW: a few safe interactions (helps tree models separate regimes) ---\n",
    "    def safe_mul(a, b): \n",
    "        return (team_tall.get(a) if a in team_tall else 0) * (team_tall.get(b) if b in team_tall else 0)\n",
    "\n",
    "    def safe_diff(a, b): \n",
    "        return (team_tall.get(a) if a in team_tall else 0) - (team_tall.get(b) if b in team_tall else 0)\n",
    "\n",
    "    # Names used below exist after delta creation; if any is missing in your run, it's treated as 0\n",
    "    team_tall['elo_synergy_product'] = safe_mul('pre_elo_mean__delta_maf_minus_cit',\n",
    "                                                'synergy_mean_team_mean__delta_maf_minus_cit')\n",
    "    team_tall['elo_enemy_gap']       = safe_diff('pre_elo_mean__delta_maf_minus_cit',\n",
    "                                                'enemy_fam_mean_team_mean__delta_maf_minus_cit')\n",
    "    team_tall['elo_streak_mix']      = safe_mul('pre_elo_mean__delta_maf_minus_cit',\n",
    "                                                'win_streak_mean__delta_maf_minus_cit')\n",
    "\n",
    "    return team_tall\n",
    "\n",
    "team_tall = build_team_agg(work_players, add_ratios=False)  # ratios often redundant\n",
    "sanity_assert_two_rows_per_game(team_tall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490c82f",
   "metadata": {},
   "source": [
    "## 6) Feature Selection\n",
    "\n",
    "We include **team-only** features and **deltas**. We **exclude** label-like columns.\n",
    "Then we create **time-aware** train/cal/test splits (70/15/15 by `game_max_id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f392194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes | X: (160564, 105) | y: (160564,)\n",
      "Split sizes | train: 136480 cal: 24084 test: 24084\n"
     ]
    }
   ],
   "source": [
    "team_only = [c for c in team_tall.columns if c.startswith((\n",
    "    'pre_elo_', 'gap_id_clipped_', 'long_break_flag_', 'place_',\n",
    "    'win_streak_', 'loss_streak_', 'synergy_mean_team_', 'synergy_max_team_',\n",
    "    'enemy_fam_', 'games_played_', \n",
    "    'don_pre_elo_role', 'sheriff_pre_elo_role', 'black_mean_pre_elo_role', 'red_mean_pre_elo_role',\n",
    "    'don_games_in_role', 'sheriff_games_in_role', 'black_mean_games_in_role', 'red_mean_games_in_role',\n",
    "    'don_wr20', 'sheriff_wr20', 'black_mean_wr20', 'red_mean_wr20',\n",
    "    'meta_period_first'\n",
    "))]\n",
    "delta_feats = [c for c in team_tall.columns if c.endswith('__delta_maf_minus_cit')]\n",
    "\n",
    "# NEW: explicitly add our interactions and meta-normalized Elo columns\n",
    "extra_feats = [c for c in ['elo_synergy_product','elo_enemy_gap','elo_streak_mix']\n",
    "               if c in team_tall.columns]\n",
    "meta_norm_feats = [c for c in team_tall.columns if c.endswith('_norm')]\n",
    "\n",
    "forbidden_tokens = {'team_win','team_win_team'}\n",
    "USED_FEATS = [c for c in sorted(set(team_only + delta_feats + extra_feats + meta_norm_feats))\n",
    "              if not any(tok in c for tok in forbidden_tokens)]\n",
    "\n",
    "X = team_tall[USED_FEATS].fillna(0)\n",
    "y = team_tall['team_win_team'].astype(int).values\n",
    "groups = team_tall['game_id'].values\n",
    "time_key = team_tall['game_max_id'].values\n",
    "\n",
    "q70, q85 = np.quantile(time_key, [0.70, 0.85])\n",
    "train_mask = time_key <= q85\n",
    "cal_mask   = (time_key > q70) & (time_key <= q85)\n",
    "test_mask  = time_key > q85\n",
    "\n",
    "print(\"Shapes | X:\", X.shape, \"| y:\", y.shape)\n",
    "print(\"Split sizes | train:\", train_mask.sum(), \"cal:\", cal_mask.sum(), \"test:\", test_mask.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11660c7",
   "metadata": {},
   "source": [
    "## 7) Model — LightGBM (calibrated)\n",
    "Train on train, calibrate on cal (sigmoid), evaluate on holdout (last 15%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d9ddd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[378]\tvalid_0's binary_logloss: 0.669491\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout (last 15%)\n",
      "LogLoss: 0.6682037215973543\n",
      "ROC-AUC: 0.6314228364649983\n",
      "Brier  : 0.23769559892684958\n"
     ]
    }
   ],
   "source": [
    "params = dict(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=127,\n",
    "    min_data_in_leaf=60,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.5,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Optional inner early stopping on last 10% of train\n",
    "tr_time = time_key[train_mask]; q90 = np.quantile(tr_time, 0.90)\n",
    "inner_tr = train_mask & (time_key <= q90)\n",
    "inner_va = train_mask & (time_key >  q90)\n",
    "\n",
    "lgb = LGBMClassifier(**params)\n",
    "lgb.fit(\n",
    "    X[inner_tr], y[inner_tr],\n",
    "    eval_set=[(X[inner_va], y[inner_va])],\n",
    "    eval_metric='logloss',\n",
    "    callbacks=[early_stopping(100), log_evaluation(0)]\n",
    ")\n",
    "\n",
    "# Freeze the already-fitted estimator (workaround for prefit deprecation)\n",
    "frozen = clone(lgb)\n",
    "frozen.__dict__.update(lgb.__dict__)\n",
    "\n",
    "# Sigmoid (Platt) calibration on the calibration slice\n",
    "calibrated = CalibratedClassifierCV(frozen, cv=\"prefit\", method=\"sigmoid\")\n",
    "calibrated.fit(X[cal_mask], y[cal_mask])\n",
    "\n",
    "p_test = calibrated.predict_proba(X[test_mask])[:,1]\n",
    "\n",
    "print(\"Holdout (last 15%)\")\n",
    "print(\"LogLoss:\", log_loss(y[test_mask], p_test))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y[test_mask], p_test))\n",
    "print(\"Brier  :\", brier_score_loss(y[test_mask], p_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199751e",
   "metadata": {},
   "source": [
    "## 8) Calibration Diagnostics\n",
    "\n",
    "Reliability curve and **ECE** (Expected Calibration Error) on holdout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fedf17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECE (10 bins): 0.0243\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ece = expected_calibration_error(y[test_mask], p_test, n_bins=10)\n",
    "print(f\"ECE (10 bins): {ece:.4f}\")\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y[test_mask], p_test, n_bins=10)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(prob_pred, prob_true, 'o-')\n",
    "plt.plot([0,1],[0,1],'--', alpha=0.6)\n",
    "plt.xlabel('Predicted probability'); plt.ylabel('Observed frequency')\n",
    "plt.title('Reliability curve (holdout)')\n",
    "plt.grid(alpha=0.2); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4020ff5",
   "metadata": {},
   "source": [
    "## 9) (Optional) CatBoost + Calibration + Blend\n",
    "\n",
    "CatBoost often complements tree ensembles. We calibrate its probabilities and optionally blend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8022c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6893339\ttest: 0.6893531\tbest: 0.6893531 (0)\ttotal: 183ms\tremaining: 7m 36s\n",
      "200:\tlearn: 0.5910732\ttest: 0.5931854\tbest: 0.5931854 (200)\ttotal: 5.03s\tremaining: 57.5s\n",
      "400:\tlearn: 0.5841623\ttest: 0.5871387\tbest: 0.5871387 (400)\ttotal: 9.69s\tremaining: 50.7s\n",
      "600:\tlearn: 0.5789110\ttest: 0.5824664\tbest: 0.5824664 (600)\ttotal: 14.3s\tremaining: 45.1s\n",
      "800:\tlearn: 0.5727988\ttest: 0.5770407\tbest: 0.5770407 (800)\ttotal: 18.8s\tremaining: 39.9s\n",
      "1000:\tlearn: 0.5670838\ttest: 0.5717026\tbest: 0.5717026 (1000)\ttotal: 23.4s\tremaining: 35s\n",
      "1200:\tlearn: 0.5619706\ttest: 0.5669323\tbest: 0.5669323 (1200)\ttotal: 27.9s\tremaining: 30.2s\n",
      "1400:\tlearn: 0.5570309\ttest: 0.5621194\tbest: 0.5621194 (1400)\ttotal: 32.4s\tremaining: 25.4s\n",
      "1600:\tlearn: 0.5522208\ttest: 0.5575466\tbest: 0.5575466 (1600)\ttotal: 37s\tremaining: 20.8s\n",
      "1800:\tlearn: 0.5479237\ttest: 0.5533899\tbest: 0.5533899 (1800)\ttotal: 41.5s\tremaining: 16.1s\n",
      "2000:\tlearn: 0.5435121\ttest: 0.5491444\tbest: 0.5491444 (2000)\ttotal: 46s\tremaining: 11.5s\n",
      "2200:\tlearn: 0.5393629\ttest: 0.5450558\tbest: 0.5450558 (2200)\ttotal: 50.6s\tremaining: 6.87s\n",
      "2400:\tlearn: 0.5352264\ttest: 0.5409782\tbest: 0.5409782 (2400)\ttotal: 55.1s\tremaining: 2.27s\n",
      "2499:\tlearn: 0.5332923\ttest: 0.5391090\tbest: 0.5391090 (2499)\ttotal: 57.4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.5391090456\n",
      "bestIteration = 2499\n",
      "\n",
      "Using CatBoost sigmoid calibration (better or equal LogLoss).\n",
      "\n",
      "CatBoost (calibrated) — Holdout\n",
      "LogLoss: 0.7064557131622529\n",
      "ROC-AUC: 0.633340696975412\n",
      "Brier  : 0.24983057849317072\n",
      "\n",
      "Blend 0.6·LGBM + 0.4·Cat — Holdout\n",
      "LogLoss: 0.6737071055552175\n",
      "ROC-AUC: 0.6344352715448871\n",
      "Brier  : 0.23973503011972425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if CATBOOST_AVAILABLE:\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_cal,   y_cal   = X[cal_mask],   y[cal_mask]\n",
    "    X_hold,  y_hold  = X[test_mask],  y[test_mask]\n",
    "\n",
    "    cat_features = []\n",
    "    if 'meta_period_first' in X.columns:\n",
    "        cat_features.append(X.columns.get_loc('meta_period_first'))\n",
    "\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features or None)\n",
    "    cal_pool   = Pool(X_cal,   y_cal,   cat_features=cat_features or None)\n",
    "    test_pool  = Pool(X_hold,  y_hold,  cat_features=cat_features or None)\n",
    "\n",
    "    cat = CatBoostClassifier(\n",
    "        iterations=2500,\n",
    "        learning_rate=0.02,\n",
    "        depth=8,\n",
    "        l2_leaf_reg=3.0,\n",
    "        random_seed=42,\n",
    "        eval_metric='Logloss',\n",
    "        loss_function='Logloss',\n",
    "        class_weights=[1.0, 2.3],\n",
    "        use_best_model=True,\n",
    "        verbose=200\n",
    "    )\n",
    "    cat.fit(train_pool, eval_set=cal_pool)\n",
    "\n",
    "   # Sigmoid (baseline)\n",
    "    cat_cal_sig = CalibratedClassifierCV(cat, cv='prefit', method='sigmoid').fit(X[cal_mask], y[cal_mask])\n",
    "    p_cat_sig = cat_cal_sig.predict_proba(X[test_mask])[:,1]\n",
    "\n",
    "    # Isotonic (may help CatBoost LogLoss if enough cal data)\n",
    "    cat_cal_iso = CalibratedClassifierCV(cat, cv='prefit', method='isotonic').fit(X[cal_mask], y[cal_mask])\n",
    "    p_cat_iso = cat_cal_iso.predict_proba(X[test_mask])[:,1]\n",
    "\n",
    "    # Pick the better calibrated CatBoost for blending (by LogLoss)\n",
    "    ll_sig = log_loss(y[test_mask], p_cat_sig)\n",
    "    ll_iso = log_loss(y[test_mask], p_cat_iso)\n",
    "    if ll_iso < ll_sig:\n",
    "        p_cat = p_cat_iso\n",
    "        print(\"Using CatBoost isotonic calibration (better LogLoss).\")\n",
    "    else:\n",
    "        p_cat = p_cat_sig\n",
    "        print(\"Using CatBoost sigmoid calibration (better or equal LogLoss).\")\n",
    "\n",
    "    print(\"\\nCatBoost (calibrated) — Holdout\")\n",
    "    print(\"LogLoss:\", log_loss(y[test_mask], p_cat))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y[test_mask], p_cat))\n",
    "    print(\"Brier  :\", brier_score_loss(y[test_mask], p_cat))\n",
    "\n",
    "    # blend\n",
    "    w_lgbm = 0.6\n",
    "    w_cat  = 0.4\n",
    "    p_blend = w_lgbm * p_test + w_cat * p_cat\n",
    "    print(f\"\\nBlend {w_lgbm:.1f}·LGBM + {w_cat:.1f}·Cat — Holdout\")\n",
    "\n",
    "    print(\"LogLoss:\", log_loss(y[test_mask], p_blend))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y[test_mask], p_blend))\n",
    "    print(\"Brier  :\", brier_score_loss(y[test_mask], p_blend))\n",
    "else:\n",
    "    print(\"CatBoost not installed — skipping optional block.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d692dc",
   "metadata": {},
   "source": [
    "### (Optional) XGBoost third learner\n",
    "A slightly different tree engine sometimes helps the ensemble by a small margin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd9899ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost not available or failed to fit: name 'p_lgbm_final' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=1200, learning_rate=0.015,\n",
    "        max_depth=7, subsample=0.9, colsample_bytree=0.9,\n",
    "        reg_lambda=1.0, reg_alpha=0.5,\n",
    "        objective='binary:logistic', eval_metric='logloss',\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    p_xgb = xgb.predict_proba(X_hold)[:,1]\n",
    "\n",
    "    # quick 3-way blend (adjust weights after checking LogLoss)\n",
    "    p_blend3 = 0.5*p_lgbm_final + 0.3*(p_cat if p_cat is not None else 0) + 0.2*p_xgb\n",
    "    print(\"\\n3-way Blend — Holdout\")\n",
    "    print(\"LogLoss:\", log_loss(y_hold, p_blend3))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_hold, p_blend3))\n",
    "    print(\"Brier  :\", brier_score_loss(y_hold, p_blend3))\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not available or failed to fit:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f72fcf",
   "metadata": {},
   "source": [
    "## 10) Summary & Next Steps\n",
    "\n",
    "- Leak-free **pre-game** predictor, calibrated probabilities.\n",
    "- Strong signals: Elo deltas/means, synergy, streaks, role history, freshness.\n",
    "- Try: different **Elo decay** `tau`, **role-pair synergy**, **CatBoost blending**, or small **Optuna** tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42deffa",
   "metadata": {},
   "source": [
    "## 11) Quick Hyper-parameter & Blend Tuning (Optuna)\n",
    "\n",
    "We tune a handful of high-impact LightGBM parameters and the blend weights\n",
    "between LGBM and CatBoost. We keep a fixed learning rate and estimators for stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c146f8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-10-11 16:59:47,661] A new study created in memory with name: no-name-c0927e53-5086-4745-9fd6-7e98f3ae250f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=55, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=55\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=55, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=55\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=55, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's binary_logloss: 0.669265\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=55, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=55\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=55, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 16:59:53,424] Trial 0 finished with value: 0.6702879063167039 and parameters: {'num_leaves': 149, 'min_data_in_leaf': 55, 'subsample': 0.8778177999440697, 'colsample_bytree': 0.9456702039773609, 'reg_lambda': 2.6320068234214364, 'reg_alpha': 0.28655211868513675, 'w_lgbm': 0.7954719223803706}. Best is trial 0 with value: 0.6702879063167039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=104, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=104\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=104, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=104\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=104, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=104\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[385]\tvalid_0's binary_logloss: 0.669673\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=104, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=104\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=104, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 16:59:58,783] Trial 1 finished with value: 0.6775063280477456 and parameters: {'num_leaves': 119, 'min_data_in_leaf': 104, 'subsample': 0.7030666875590743, 'colsample_bytree': 0.9728503450743742, 'reg_lambda': 2.853594435575606, 'reg_alpha': 0.3935870309038906, 'w_lgbm': 0.4630427923215505}. Best is trial 0 with value: 0.6702879063167039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=61, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=61\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=61, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=61\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=61, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[423]\tvalid_0's binary_logloss: 0.669956\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=61, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=61\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=61, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:03,868] Trial 2 finished with value: 0.6720315857305547 and parameters: {'num_leaves': 111, 'min_data_in_leaf': 61, 'subsample': 0.7116117849869308, 'colsample_bytree': 0.8223938458514819, 'reg_lambda': 2.6718834090303365, 'reg_alpha': 2.541119044174989, 'w_lgbm': 0.6571693831668309}. Best is trial 0 with value: 0.6702879063167039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=96, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=96\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=96, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=96\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=96, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[365]\tvalid_0's binary_logloss: 0.669933\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=96, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=96\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=96, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:08,428] Trial 3 finished with value: 0.6722063384592516 and parameters: {'num_leaves': 99, 'min_data_in_leaf': 96, 'subsample': 0.8580504300876995, 'colsample_bytree': 0.9472205508341092, 'reg_lambda': 2.9459941454726537, 'reg_alpha': 1.7492164394997305, 'w_lgbm': 0.5940884697434194}. Best is trial 0 with value: 0.6702879063167039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:12,784] Trial 4 finished with value: 0.6733304837176559 and parameters: {'num_leaves': 119, 'min_data_in_leaf': 115, 'subsample': 0.897144550811581, 'colsample_bytree': 0.9861260820295046, 'reg_lambda': 1.6887299720679558, 'reg_alpha': 2.1717444169837687, 'w_lgbm': 0.5403939910211577}. Best is trial 0 with value: 0.6702879063167039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[278]\tvalid_0's binary_logloss: 0.669654\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=65, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=65\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=65, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=65\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=65, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[378]\tvalid_0's binary_logloss: 0.66949\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=65, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=65\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=65, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:17,410] Trial 5 finished with value: 0.6757075500320859 and parameters: {'num_leaves': 115, 'min_data_in_leaf': 65, 'subsample': 0.7628256514821612, 'colsample_bytree': 0.767625908483716, 'reg_lambda': 2.674658000319066, 'reg_alpha': 0.009809361678845385, 'w_lgbm': 0.517040078791344}. Best is trial 0 with value: 0.6702879063167039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:22,158] Trial 6 finished with value: 0.6699984919771609 and parameters: {'num_leaves': 143, 'min_data_in_leaf': 119, 'subsample': 0.9190320113420472, 'colsample_bytree': 0.9301432767918967, 'reg_lambda': 2.2405225092110257, 'reg_alpha': 1.6390300507453017, 'w_lgbm': 0.7123495615953888}. Best is trial 6 with value: 0.6699984919771609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's binary_logloss: 0.669648\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:26,398] Trial 7 finished with value: 0.676910625460669 and parameters: {'num_leaves': 116, 'min_data_in_leaf': 129, 'subsample': 0.7247912343764197, 'colsample_bytree': 0.8757639916505358, 'reg_lambda': 0.8808620281911324, 'reg_alpha': 2.6478744674912296, 'w_lgbm': 0.4487709475967487}. Best is trial 6 with value: 0.6699984919771609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[312]\tvalid_0's binary_logloss: 0.669972\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[457]\tvalid_0's binary_logloss: 0.670081\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:30,455] Trial 8 finished with value: 0.6672874043243403 and parameters: {'num_leaves': 76, 'min_data_in_leaf': 127, 'subsample': 0.7812162436251557, 'colsample_bytree': 0.7968558242057955, 'reg_lambda': 0.40619360902901613, 'reg_alpha': 0.19464019252917275, 'w_lgbm': 0.8732430209357275}. Best is trial 8 with value: 0.6672874043243403.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:34,395] Trial 9 finished with value: 0.673315179529166 and parameters: {'num_leaves': 70, 'min_data_in_leaf': 52, 'subsample': 0.7246610333400506, 'colsample_bytree': 0.8972882174524941, 'reg_lambda': 1.496379037937385, 'reg_alpha': 0.4758808651411035, 'w_lgbm': 0.5239512925266221}. Best is trial 8 with value: 0.6672874043243403.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[426]\tvalid_0's binary_logloss: 0.670255\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=52, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=52\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[531]\tvalid_0's binary_logloss: 0.670373\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:39,222] Trial 10 finished with value: 0.6671894150287239 and parameters: {'num_leaves': 65, 'min_data_in_leaf': 139, 'subsample': 0.8153181206510485, 'colsample_bytree': 0.7552114442679237, 'reg_lambda': 0.0927388280770357, 'reg_alpha': 0.9924905803343984, 'w_lgbm': 0.89540321119558}. Best is trial 10 with value: 0.6671894150287239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=133, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=133\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=133, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=133\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=133, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=133\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[709]\tvalid_0's binary_logloss: 0.670331\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=133, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=133\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=133, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:44,355] Trial 11 finished with value: 0.6680843634263043 and parameters: {'num_leaves': 63, 'min_data_in_leaf': 133, 'subsample': 0.8030336381515302, 'colsample_bytree': 0.7524281584046542, 'reg_lambda': 0.037746113829414285, 'reg_alpha': 0.9890443188884059, 'w_lgbm': 0.8921926531381046}. Best is trial 10 with value: 0.6671894150287239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:47,824] Trial 12 finished with value: 0.6667443404917526 and parameters: {'num_leaves': 83, 'min_data_in_leaf': 140, 'subsample': 0.8005350593397594, 'colsample_bytree': 0.8053461463248702, 'reg_lambda': 0.0633662331183157, 'reg_alpha': 0.9325578423454974, 'w_lgbm': 0.8747022822693388}. Best is trial 12 with value: 0.6667443404917526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[315]\tvalid_0's binary_logloss: 0.670175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[437]\tvalid_0's binary_logloss: 0.67009\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:52,061] Trial 13 finished with value: 0.6682701642561294 and parameters: {'num_leaves': 84, 'min_data_in_leaf': 139, 'subsample': 0.8333893851890392, 'colsample_bytree': 0.826884611591746, 'reg_lambda': 0.734460782412593, 'reg_alpha': 0.9727127372447347, 'w_lgbm': 0.8027031592131345}. Best is trial 12 with value: 0.6667443404917526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[577]\tvalid_0's binary_logloss: 0.669551\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:00:57,367] Trial 14 finished with value: 0.6702395264233708 and parameters: {'num_leaves': 90, 'min_data_in_leaf': 80, 'subsample': 0.8256695450528747, 'colsample_bytree': 0.7968417524830147, 'reg_lambda': 0.013879291117899317, 'reg_alpha': 1.0574828482274992, 'w_lgbm': 0.7873647617857317}. Best is trial 12 with value: 0.6667443404917526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=110, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=110\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=110, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=110\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=110, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[475]\tvalid_0's binary_logloss: 0.669817\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=110, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=110\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=110, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:02,267] Trial 15 finished with value: 0.6687985600637552 and parameters: {'num_leaves': 96, 'min_data_in_leaf': 110, 'subsample': 0.7615563740007679, 'colsample_bytree': 0.8400526608200324, 'reg_lambda': 1.0519720766407956, 'reg_alpha': 1.3389125312413666, 'w_lgbm': 0.8464843275646171}. Best is trial 12 with value: 0.6667443404917526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[436]\tvalid_0's binary_logloss: 0.669973\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:06,916] Trial 16 finished with value: 0.6692045151873273 and parameters: {'num_leaves': 79, 'min_data_in_leaf': 80, 'subsample': 0.798567663561206, 'colsample_bytree': 0.7823489231649665, 'reg_lambda': 0.44639047492421685, 'reg_alpha': 0.7240933245052614, 'w_lgbm': 0.7338974153012283}. Best is trial 12 with value: 0.6667443404917526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[659]\tvalid_0's binary_logloss: 0.670092\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=138, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:11,946] Trial 17 finished with value: 0.6701089297906622 and parameters: {'num_leaves': 65, 'min_data_in_leaf': 138, 'subsample': 0.8515002484031007, 'colsample_bytree': 0.8529396583116949, 'reg_lambda': 1.3789573698191933, 'reg_alpha': 1.3475409771722755, 'w_lgbm': 0.7200296710176732}. Best is trial 12 with value: 0.6667443404917526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:15,810] Trial 18 finished with value: 0.6678948560637294 and parameters: {'num_leaves': 101, 'min_data_in_leaf': 126, 'subsample': 0.9440833995419893, 'colsample_bytree': 0.7508859413785877, 'reg_lambda': 0.3879453277959132, 'reg_alpha': 1.9589652451719801, 'w_lgbm': 0.8265165792150022}. Best is trial 12 with value: 0.6667443404917526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[322]\tvalid_0's binary_logloss: 0.669776\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=126, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=126\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=99, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=99\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=99, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=99\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=99, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=99\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:20,965] Trial 19 finished with value: 0.671819431139676 and parameters: {'num_leaves': 135, 'min_data_in_leaf': 99, 'subsample': 0.7498409712554994, 'colsample_bytree': 0.8081837668170814, 'reg_lambda': 1.9828223555538138, 'reg_alpha': 0.8280866642272678, 'w_lgbm': 0.6526895752988264}. Best is trial 12 with value: 0.6667443404917526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[313]\tvalid_0's binary_logloss: 0.66942\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=99, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=99\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=99, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=99\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=81, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=81\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=81, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=81\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=81, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[490]\tvalid_0's binary_logloss: 0.669758\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=81, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=81\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=81, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:26,140] Trial 20 finished with value: 0.6679482313743139 and parameters: {'num_leaves': 86, 'min_data_in_leaf': 81, 'subsample': 0.8048515041331871, 'colsample_bytree': 0.8607392468233231, 'reg_lambda': 1.105718174170926, 'reg_alpha': 0.6197723889204998, 'w_lgbm': 0.8915604863298618}. Best is trial 12 with value: 0.6667443404917526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=123, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=123\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=123, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=123\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=123, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=123\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:29,797] Trial 21 finished with value: 0.6667325415212769 and parameters: {'num_leaves': 75, 'min_data_in_leaf': 123, 'subsample': 0.7832897360708728, 'colsample_bytree': 0.7866295771711799, 'reg_lambda': 0.4120619599023918, 'reg_alpha': 1.2894899365250985, 'w_lgbm': 0.8626695342206523}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[352]\tvalid_0's binary_logloss: 0.670124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=123, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=123\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=123, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=123\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[527]\tvalid_0's binary_logloss: 0.669992\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:34,371] Trial 22 finished with value: 0.6690444170613604 and parameters: {'num_leaves': 73, 'min_data_in_leaf': 121, 'subsample': 0.7847990417571508, 'colsample_bytree': 0.7755361641319708, 'reg_lambda': 0.2213310218511606, 'reg_alpha': 1.2413525825265057, 'w_lgbm': 0.765082139333608}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:37,860] Trial 23 finished with value: 0.6670178659158067 and parameters: {'num_leaves': 78, 'min_data_in_leaf': 139, 'subsample': 0.8390930844961086, 'colsample_bytree': 0.7848408179811442, 'reg_lambda': 0.6292753946719332, 'reg_alpha': 1.4273826256942777, 'w_lgbm': 0.8361253181927962}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[338]\tvalid_0's binary_logloss: 0.670204\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[594]\tvalid_0's binary_logloss: 0.669614\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=109, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:43,341] Trial 24 finished with value: 0.6698696885884813 and parameters: {'num_leaves': 91, 'min_data_in_leaf': 109, 'subsample': 0.8436231588364572, 'colsample_bytree': 0.815143991940156, 'reg_lambda': 0.6600988653546012, 'reg_alpha': 1.600217452435888, 'w_lgbm': 0.8404903806150157}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[467]\tvalid_0's binary_logloss: 0.67022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:47,607] Trial 25 finished with value: 0.6674949666001272 and parameters: {'num_leaves': 79, 'min_data_in_leaf': 134, 'subsample': 0.8715766768122983, 'colsample_bytree': 0.7927325199822868, 'reg_lambda': 0.5877274941654931, 'reg_alpha': 1.9469963457484694, 'w_lgbm': 0.8518376331949101}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[368]\tvalid_0's binary_logloss: 0.669725\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:52,083] Trial 26 finished with value: 0.6691847851941661 and parameters: {'num_leaves': 105, 'min_data_in_leaf': 121, 'subsample': 0.7510315980282433, 'colsample_bytree': 0.8354556525214912, 'reg_lambda': 1.1920664771633838, 'reg_alpha': 1.2517691583369779, 'w_lgbm': 0.7601599749931053}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[505]\tvalid_0's binary_logloss: 0.670169\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:01:56,777] Trial 27 finished with value: 0.6704270350649155 and parameters: {'num_leaves': 84, 'min_data_in_leaf': 130, 'subsample': 0.7823498891346816, 'colsample_bytree': 0.7753381987627982, 'reg_lambda': 0.3018204288287789, 'reg_alpha': 2.2207285804820005, 'w_lgbm': 0.6954072080368632}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[693]\tvalid_0's binary_logloss: 0.669996\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:02,360] Trial 28 finished with value: 0.669098758621667 and parameters: {'num_leaves': 71, 'min_data_in_leaf': 140, 'subsample': 0.8304534936480754, 'colsample_bytree': 0.8881126935827837, 'reg_lambda': 0.8166395929951853, 'reg_alpha': 1.4682902355678702, 'w_lgbm': 0.8229995538521531}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[491]\tvalid_0's binary_logloss: 0.669625\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=44, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:08,336] Trial 29 finished with value: 0.6720882228019102 and parameters: {'num_leaves': 132, 'min_data_in_leaf': 44, 'subsample': 0.8908618594221072, 'colsample_bytree': 0.8034634640426128, 'reg_lambda': 0.5588953355752986, 'reg_alpha': 2.981062336175558, 'w_lgbm': 0.7710874342799785}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[427]\tvalid_0's binary_logloss: 0.669667\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:12,873] Trial 30 finished with value: 0.6683206132972078 and parameters: {'num_leaves': 94, 'min_data_in_leaf': 124, 'subsample': 0.8708334264641001, 'colsample_bytree': 0.9105577405192472, 'reg_lambda': 0.18322343261866425, 'reg_alpha': 1.1255267424041584, 'w_lgbm': 0.8081158510267495}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[658]\tvalid_0's binary_logloss: 0.670173\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:17,726] Trial 31 finished with value: 0.6684880744013825 and parameters: {'num_leaves': 68, 'min_data_in_leaf': 140, 'subsample': 0.8163837771881225, 'colsample_bytree': 0.7644473279923154, 'reg_lambda': 0.010830849053999135, 'reg_alpha': 0.801344112920995, 'w_lgbm': 0.8673527798859095}. Best is trial 21 with value: 0.6667325415212769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:21,015] Trial 32 finished with value: 0.6665033737978745 and parameters: {'num_leaves': 78, 'min_data_in_leaf': 134, 'subsample': 0.8140936140036887, 'colsample_bytree': 0.7844939514101106, 'reg_lambda': 0.1904075276204348, 'reg_alpha': 0.5453556057858624, 'w_lgbm': 0.8853110819376774}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[319]\tvalid_0's binary_logloss: 0.670257\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=114, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=114\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=114, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=114\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=114, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=114\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[472]\tvalid_0's binary_logloss: 0.670139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=114, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=114\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=114, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:25,287] Trial 33 finished with value: 0.6678242705985008 and parameters: {'num_leaves': 80, 'min_data_in_leaf': 114, 'subsample': 0.7905354703348896, 'colsample_bytree': 0.7852081481535501, 'reg_lambda': 0.2944933235637514, 'reg_alpha': 0.5499868483163135, 'w_lgbm': 0.8595111091980658}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:28,793] Trial 34 finished with value: 0.6672041229194652 and parameters: {'num_leaves': 86, 'min_data_in_leaf': 134, 'subsample': 0.7728571820718865, 'colsample_bytree': 0.8167620665861407, 'reg_lambda': 0.5180424788461451, 'reg_alpha': 1.4777928397462903, 'w_lgbm': 0.8224637500305981}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[314]\tvalid_0's binary_logloss: 0.670063\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[372]\tvalid_0's binary_logloss: 0.669774\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=132, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:33,184] Trial 35 finished with value: 0.6731332094845568 and parameters: {'num_leaves': 106, 'min_data_in_leaf': 132, 'subsample': 0.8385884262785128, 'colsample_bytree': 0.8410418687453006, 'reg_lambda': 0.9028621765544254, 'reg_alpha': 0.16757667077979765, 'w_lgbm': 0.5782011069843686}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[455]\tvalid_0's binary_logloss: 0.670151\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=106, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:37,151] Trial 36 finished with value: 0.670188526775576 and parameters: {'num_leaves': 75, 'min_data_in_leaf': 106, 'subsample': 0.8142975245294765, 'colsample_bytree': 0.7693810327768256, 'reg_lambda': 0.19110368041293696, 'reg_alpha': 0.826880032401812, 'w_lgbm': 0.6770893101872744}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[505]\tvalid_0's binary_logloss: 0.669946\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:42,004] Trial 37 finished with value: 0.6684624404559285 and parameters: {'num_leaves': 91, 'min_data_in_leaf': 98, 'subsample': 0.8595517663925211, 'colsample_bytree': 0.7902278770440415, 'reg_lambda': 0.6712569799113695, 'reg_alpha': 0.3777960008889183, 'w_lgbm': 0.8987868872226265}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[461]\tvalid_0's binary_logloss: 0.669781\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=115, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:46,712] Trial 38 finished with value: 0.6778483635600214 and parameters: {'num_leaves': 81, 'min_data_in_leaf': 115, 'subsample': 0.888963226120655, 'colsample_bytree': 0.962881253101558, 'reg_lambda': 1.794843623380808, 'reg_alpha': 1.800425677937741, 'w_lgbm': 0.42486055305320103}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[514]\tvalid_0's binary_logloss: 0.670004\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:51,068] Trial 39 finished with value: 0.6690992692899548 and parameters: {'num_leaves': 73, 'min_data_in_leaf': 88, 'subsample': 0.7707719010576933, 'colsample_bytree': 0.825843046165885, 'reg_lambda': 0.31439962866337956, 'reg_alpha': 0.6393046083847409, 'w_lgbm': 0.7483695855122356}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=125, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=125\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=125, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=125\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=125, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=125\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[371]\tvalid_0's binary_logloss: 0.669712\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=125, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=125\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=125, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:02:55,976] Trial 40 finished with value: 0.6696534404759893 and parameters: {'num_leaves': 128, 'min_data_in_leaf': 125, 'subsample': 0.7487588468043328, 'colsample_bytree': 0.8090925060559995, 'reg_lambda': 2.360426055577106, 'reg_alpha': 0.02472038401192811, 'w_lgbm': 0.7921495840784353}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[744]\tvalid_0's binary_logloss: 0.67009\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:03:01,429] Trial 41 finished with value: 0.668950863938837 and parameters: {'num_leaves': 67, 'min_data_in_leaf': 134, 'subsample': 0.8151836984353151, 'colsample_bytree': 0.7619982801531233, 'reg_lambda': 0.14415820983623895, 'reg_alpha': 1.1093931784452393, 'w_lgbm': 0.8803976341826316}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=136, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=136\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=136, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=136\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=136, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=136\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[394]\tvalid_0's binary_logloss: 0.669587\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=136, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=136\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=136, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:03:07,254] Trial 42 finished with value: 0.6747619925888065 and parameters: {'num_leaves': 152, 'min_data_in_leaf': 136, 'subsample': 0.8037744577350204, 'colsample_bytree': 0.7580112589001301, 'reg_lambda': 0.12407385232065771, 'reg_alpha': 0.8596383879258068, 'w_lgbm': 0.620897027667007}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[570]\tvalid_0's binary_logloss: 0.67017\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:03:11,580] Trial 43 finished with value: 0.6674870044714942 and parameters: {'num_leaves': 63, 'min_data_in_leaf': 129, 'subsample': 0.8191709895132769, 'colsample_bytree': 0.7820457700305662, 'reg_lambda': 0.4295320078755647, 'reg_alpha': 0.3724258549894058, 'w_lgbm': 0.8668318694223001}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:03:15,278] Trial 44 finished with value: 0.667093477631406 and parameters: {'num_leaves': 76, 'min_data_in_leaf': 119, 'subsample': 0.7927910724738401, 'colsample_bytree': 0.7715811423698044, 'reg_lambda': 0.01802171585426253, 'reg_alpha': 0.9314584066666416, 'w_lgbm': 0.896068024718568}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[401]\tvalid_0's binary_logloss: 0.670275\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=119, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=119\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[525]\tvalid_0's binary_logloss: 0.670173\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=117, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:03:19,761] Trial 45 finished with value: 0.6681661146492047 and parameters: {'num_leaves': 76, 'min_data_in_leaf': 117, 'subsample': 0.7961131966180686, 'colsample_bytree': 0.8003460148761804, 'reg_lambda': 0.2704021416088754, 'reg_alpha': 1.6407495277507018, 'w_lgbm': 0.8417298722839013}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=122, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=122\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=122, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=122\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=122, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=122\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:03:23,325] Trial 46 finished with value: 0.6670319982584899 and parameters: {'num_leaves': 89, 'min_data_in_leaf': 122, 'subsample': 0.7350527099199381, 'colsample_bytree': 0.7709112769463538, 'reg_lambda': 0.46855704770963713, 'reg_alpha': 1.2312664195670147, 'w_lgbm': 0.8723781080313682}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[318]\tvalid_0's binary_logloss: 0.669885\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=122, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=122\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=122, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=122\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[371]\tvalid_0's binary_logloss: 0.66954\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=129, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:03:28,225] Trial 47 finished with value: 0.6679777977387483 and parameters: {'num_leaves': 100, 'min_data_in_leaf': 129, 'subsample': 0.705466599495987, 'colsample_bytree': 0.993597758892878, 'reg_lambda': 0.8258531414502079, 'reg_alpha': 1.2628121514035076, 'w_lgbm': 0.8146736231569444}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015698 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[522]\tvalid_0's binary_logloss: 0.669877\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:03:33,990] Trial 48 finished with value: 0.6768977101537973 and parameters: {'num_leaves': 87, 'min_data_in_leaf': 124, 'subsample': 0.7290906522207532, 'colsample_bytree': 0.7888183530589333, 'reg_lambda': 0.49971264460159903, 'reg_alpha': 1.424248603376943, 'w_lgbm': 0.47945310106770633}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[491]\tvalid_0's binary_logloss: 0.669838\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=68, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "[I 2025-10-11 17:03:38,411] Trial 49 finished with value: 0.6677581677150443 and parameters: {'num_leaves': 82, 'min_data_in_leaf': 68, 'subsample': 0.7321495170194401, 'colsample_bytree': 0.8550610050085332, 'reg_lambda': 1.007998590530824, 'reg_alpha': 1.1409134020164275, 'w_lgbm': 0.8705516559374902}. Best is trial 32 with value: 0.6665033737978745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LogLoss: 0.6665033737978745\n",
      "Best params: {'num_leaves': 78, 'min_data_in_leaf': 134, 'subsample': 0.8140936140036887, 'colsample_bytree': 0.7844939514101106, 'reg_lambda': 0.1904075276204348, 'reg_alpha': 0.5453556057858624, 'w_lgbm': 0.8853110819376774}\n",
      "Best AUC (attr): 0.6329406825322467\n",
      "Best Brier (attr): 0.23694427560003406\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] Number of positive: 61416, number of negative: 61416\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15097\n",
      "[LightGBM] [Info] Number of data points in the train set: 122832, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[319]\tvalid_0's binary_logloss: 0.670257\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "\n",
      "Final blend weights: LGBM=0.89, Cat=0.11\n",
      "\n",
      "Tuned — Holdout\n",
      "LogLoss: 0.6665033737978745\n",
      "ROC-AUC: 0.6329406825322467\n",
      "Brier  : 0.23694427560003406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\data_analysis\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.base import clone\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_cal,   y_cal   = X[cal_mask],   y[cal_mask]\n",
    "X_hold,  y_hold  = X[test_mask],  y[test_mask]\n",
    "\n",
    "def train_lgbm_with_params(params_dict):\n",
    "    lgb = LGBMClassifier(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=int(params_dict['num_leaves']),\n",
    "        min_data_in_leaf=int(params_dict['min_data_in_leaf']),\n",
    "        subsample=float(params_dict['subsample']),\n",
    "        colsample_bytree=float(params_dict['colsample_bytree']),\n",
    "        reg_lambda=float(params_dict['reg_lambda']),\n",
    "        reg_alpha=float(params_dict['reg_alpha']),\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    # inner early-stop on last 10% of train time\n",
    "    tr_t = time_key[train_mask]; q90 = np.quantile(tr_t, 0.90)\n",
    "    inner_tr = (time_key <= q90) & train_mask\n",
    "    inner_va = (time_key >  q90) & train_mask\n",
    "\n",
    "    lgb.fit(\n",
    "        X[inner_tr], y[inner_tr],\n",
    "        eval_set=[(X[inner_va], y[inner_va])],\n",
    "        eval_metric='logloss',\n",
    "        callbacks=[early_stopping(100), log_evaluation(0)]\n",
    "    )\n",
    "    # Calibrate on cal split (sigmoid)\n",
    "    fr = clone(lgb); fr.__dict__.update(lgb.__dict__)\n",
    "    cal = CalibratedClassifierCV(fr, cv='prefit', method='sigmoid')\n",
    "    cal.fit(X_cal, y_cal)\n",
    "    p = cal.predict_proba(X_hold)[:,1]\n",
    "    return p, lgb\n",
    "\n",
    "# Cache CatBoost predictions from your earlier section (if available)\n",
    "# If not run yet, train a small Cat here:\n",
    "if CATBOOST_AVAILABLE:\n",
    "    try:\n",
    "        p_cat  # exists\n",
    "    except NameError:\n",
    "        train_pool = Pool(X_train, y_train)\n",
    "        cal_pool   = Pool(X_cal,   y_cal)\n",
    "        cat = CatBoostClassifier(\n",
    "            iterations=2000, learning_rate=0.02, depth=8,\n",
    "            l2_leaf_reg=3.0, random_seed=42,\n",
    "            eval_metric='Logloss', loss_function='Logloss',\n",
    "            auto_class_weights='Balanced',\n",
    "            use_best_model=True, verbose=False\n",
    "        )\n",
    "        cat.fit(train_pool, eval_set=cal_pool, verbose=False)\n",
    "        cat_cal = CalibratedClassifierCV(cat, cv='prefit', method='sigmoid').fit(X_cal, y_cal)\n",
    "        p_cat = cat_cal.predict_proba(X_hold)[:,1]\n",
    "else:\n",
    "    p_cat = None  # tuning will ignore blend weight\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    # LGBM params to tune (narrow, safe ranges)\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 63, 159),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 40, 140),\n",
    "        'subsample': trial.suggest_float('subsample', 0.70, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.75, 1.00),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 3.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 3.0),\n",
    "    }\n",
    "    p_lgbm, _ = train_lgbm_with_params(params)\n",
    "\n",
    "    if p_cat is not None:\n",
    "        w_lgbm = trial.suggest_float('w_lgbm', 0.4, 0.9)\n",
    "        w_cat  = 1.0 - w_lgbm\n",
    "        p_blend = w_lgbm * p_lgbm + w_cat * p_cat\n",
    "        loss = log_loss(y_hold, p_blend)\n",
    "    else:\n",
    "        loss = log_loss(y_hold, p_lgbm)\n",
    "\n",
    "    # report auxiliary metrics\n",
    "    trial.set_user_attr('AUC', roc_auc_score(y_hold, p_lgbm if p_cat is None else p_blend))\n",
    "    trial.set_user_attr('Brier', brier_score_loss(y_hold, p_lgbm if p_cat is None else p_blend))\n",
    "    return loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "print(\"Best LogLoss:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best AUC (attr):\", study.best_trial.user_attrs.get('AUC'))\n",
    "print(\"Best Brier (attr):\", study.best_trial.user_attrs.get('Brier'))\n",
    "\n",
    "# Refit final LGBM with best params and produce final blended predictions\n",
    "best_params = study.best_params.copy()\n",
    "w_lgbm = best_params.pop('w_lgbm', 1.0)  # present only if CatBoost available\n",
    "p_lgbm_final, lgb_final = train_lgbm_with_params(best_params)\n",
    "\n",
    "if p_cat is not None:\n",
    "    w_cat = 1.0 - w_lgbm\n",
    "    p_final = w_lgbm * p_lgbm_final + w_cat * p_cat\n",
    "    print(f\"\\nFinal blend weights: LGBM={w_lgbm:.2f}, Cat={w_cat:.2f}\")\n",
    "else:\n",
    "    p_final = p_lgbm_final\n",
    "\n",
    "print(\"\\nTuned — Holdout\")\n",
    "print(\"LogLoss:\", log_loss(y_hold, p_final))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_hold, p_final))\n",
    "print(\"Brier  :\", brier_score_loss(y_hold, p_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531dae28",
   "metadata": {},
   "source": [
    "### (Optional) Micro-sweep of Elo decay τ\n",
    "Quick check around τ=400 to confirm the best setting on this dataset split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39f6d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taus = [300, 350, 400, 450, 500]\n",
    "# results = []\n",
    "# for t in taus:\n",
    "#     # --- rebuild features with this tau ---\n",
    "#     wp = compute_elos(df, init=1500, k=24, tau=float(t))\n",
    "#     wp = add_rolling_stats_side(wp)\n",
    "#     wp = add_role_history_stats(wp, windows=(5,20,50))\n",
    "#     wp = add_synergy_features(wp)\n",
    "#     wp = add_enemy_familiarity_features(wp)\n",
    "#     wp = add_streak_features(wp)\n",
    "#     wp = add_games_played_feature(wp)\n",
    "#     team_tall = build_team_agg(wp, add_ratios=False)\n",
    "\n",
    "#     # feature selection (same logic as main flow)\n",
    "#     team_only = [c for c in team_tall.columns if c.startswith((\n",
    "#         'pre_elo_', 'gap_id_clipped_', 'long_break_flag_', 'place_',\n",
    "#         'win_streak_', 'loss_streak_', 'synergy_mean_team_', 'synergy_max_team_',\n",
    "#         'enemy_fam_', 'games_played_', \n",
    "#         'don_pre_elo_role', 'sheriff_pre_elo_role', 'black_mean_pre_elo_role', 'red_mean_pre_elo_role',\n",
    "#         'don_games_in_role', 'sheriff_games_in_role', 'black_mean_games_in_role', 'red_mean_games_in_role',\n",
    "#         'don_wr20', 'sheriff_wr20', 'black_mean_wr20', 'red_mean_wr20',\n",
    "#         'meta_period_first'\n",
    "#     ))]\n",
    "#     delta_feats = [c for c in team_tall.columns if c.endswith('__delta_maf_minus_cit')]\n",
    "#     extra_feats = [c for c in ['elo_synergy_product','elo_enemy_gap','elo_streak_mix'] if c in team_tall.columns]\n",
    "#     meta_norm_feats = [c for c in team_tall.columns if c.endswith('_norm')]\n",
    "\n",
    "#     forbidden_tokens = {'team_win','team_win_team'}\n",
    "#     USED_FEATS = [c for c in sorted(set(team_only + delta_feats + extra_feats + meta_norm_feats))\n",
    "#                   if not any(tok in c for tok in forbidden_tokens)]\n",
    "\n",
    "#     X = team_tall[USED_FEATS].fillna(0).values\n",
    "#     y = team_tall['team_win_team'].astype(int).values\n",
    "#     time_key = team_tall['game_max_id'].values\n",
    "\n",
    "#     # time-aware split\n",
    "#     q70, q85 = np.quantile(time_key, [0.70, 0.85])\n",
    "#     train_mask = time_key <= q85\n",
    "#     cal_mask   = (time_key > q70) & (time_key <= q85)\n",
    "#     test_mask  = time_key > q85\n",
    "\n",
    "#     # inner early-stop split (last 10% of train)\n",
    "#     tr_time = time_key[train_mask]\n",
    "#     q90 = np.quantile(tr_time, 0.90)\n",
    "#     inner_tr = train_mask & (time_key <= q90)\n",
    "#     inner_va = train_mask & (time_key >  q90)\n",
    "\n",
    "#     # model\n",
    "#     lgb = LGBMClassifier(\n",
    "#         n_estimators=1500, learning_rate=0.01,\n",
    "#         num_leaves=127, min_data_in_leaf=60,\n",
    "#         subsample=0.9, colsample_bytree=0.9,\n",
    "#         reg_lambda=1.0, reg_alpha=0.5,\n",
    "#         class_weight='balanced', random_state=42, n_jobs=-1\n",
    "#     )\n",
    "#     lgb.fit(\n",
    "#         X[inner_tr], y[inner_tr],\n",
    "#         eval_set=[(X[inner_va], y[inner_va])],\n",
    "#         eval_metric='logloss',\n",
    "#         callbacks=[early_stopping(100), log_evaluation(0)]\n",
    "#     )\n",
    "\n",
    "#     # calibration on cal split — use the *fitted* estimator directly\n",
    "#     cal = CalibratedClassifierCV(lgb, cv='prefit', method='sigmoid').fit(X[cal_mask], y[cal_mask])\n",
    "#     p = cal.predict_proba(X[test_mask])[:,1]\n",
    "\n",
    "#     auc = roc_auc_score(y[test_mask], p)\n",
    "#     ll  = log_loss(y[test_mask], p)\n",
    "#     print(f\"tau={t}  AUC={auc:.4f}  LogLoss={ll:.4f}\")\n",
    "#     results.append((t, auc, ll))\n",
    "\n",
    "# # pick best by LogLoss\n",
    "# best = min(results, key=lambda x: x[2])\n",
    "# print(\"Best (by LogLoss):\", best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
