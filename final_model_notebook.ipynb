{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa6397b",
   "metadata": {},
   "source": [
    "# 🎓 Mafia Game — Pre-Game Winner Prediction\n",
    "\n",
    "This notebook builds a **leak-free, pre-game prediction model** for Mafia games.\n",
    "We go from **clean per-player rows** to **per-game team probabilities** using:\n",
    "- Temporal **Elo with decay** (skills evolve over time)\n",
    "- **Role & side** rolling performance\n",
    "- **Breaks**/freshness\n",
    "- **Role-specific history** (experience & win rates on each role)\n",
    "- **Synergy** (same-team familiarity) & **Enemy familiarity** (cross-team history)\n",
    "- **Streaks** (win/loss momentum)\n",
    "- **Meta-eras** (ruleset changes over time)\n",
    "- **Team aggregation** → opponent deltas\n",
    "- **LightGBM** (main) and optional **CatBoost** (comparison), with **probability calibration**\n",
    "- Proper **time-aware evaluation** (holdout = last 15% by time proxy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66121bb5",
   "metadata": {},
   "source": [
    "## 1) Environment & Imports\n",
    "\n",
    "If something is missing, install via the first cell. Then import everything we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bb3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, uncomment to install packages\n",
    "# !pip install -q lightgbm catboost scikit-learn optuna matplotlib pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Modeling & metrics\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Optional\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6a096",
   "metadata": {},
   "source": [
    "## 2) Configuration & Data Load\n",
    "\n",
    "- **Input:** a cleaned per-player table (one row per player per game).  \n",
    "  Columns required (min): `id, game_id, player_id, role, team, place, game_points, team_win`  \n",
    "- **Assumptions:** each game has **10 players**; exactly **one team wins** (7 winners if citizens, 3 if mafia).  \n",
    "- **Time proxy:** `id` increases with time.\n",
    "\n",
    "> Update `DATA_CSV` if your file is in a different location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97a8a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (802820, 21) columns: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_44808\\712523340.py:4: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_CSV)\n"
     ]
    }
   ],
   "source": [
    "DATA_CSV = Path(\"cleaned/mafia_clean.csv\")   # put the CSV next to this notebook or provide an absolute path\n",
    "OUT_DIR  = Path(\"cleaned\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print(\"Loaded:\", df.shape, \"columns:\", len(df.columns))\n",
    "assert {'id','game_id','player_id','role','team','game_points','team_win'}.issubset(df.columns), \\\n",
    "    \"Missing required columns in the cleaned dataset.\"\n",
    "\n",
    "# Basic coercions\n",
    "df['id'] = pd.to_numeric(df['id'], errors='coerce').astype('int64')\n",
    "df['game_id'] = pd.to_numeric(df['game_id'], errors='coerce').astype('int64')\n",
    "df['player_id'] = pd.to_numeric(df['player_id'], errors='coerce').astype('int64')\n",
    "df['team_win'] = pd.to_numeric(df['team_win'], errors='coerce').astype('int8')\n",
    "df['team'] = df['team'].astype('category')\n",
    "df['role'] = df['role'].astype('category')\n",
    "\n",
    "# Seat/position optional column name normalization (if present)\n",
    "if 'place' in df.columns:\n",
    "    df['place'] = pd.to_numeric(df['place'], errors='coerce').fillna(0).astype('int16')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec477f",
   "metadata": {},
   "source": [
    "## 3) Helper Utilities\n",
    "\n",
    "Small helpers for quantiles and sanity checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79594864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q25(x): return np.nanpercentile(x, 25)\n",
    "def q75(x): return np.nanpercentile(x, 75)\n",
    "\n",
    "def sanity_assert_two_rows_per_game(team_tall):\n",
    "    cnt = team_tall.groupby('game_id').size().value_counts()\n",
    "    print(\"Rows per game distribution:\\n\", cnt.head())\n",
    "    assert 2 in cnt.index.tolist(), \"Every game should have exactly 2 rows (one per team).\"\n",
    "\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    inds = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        mask = inds == b\n",
    "        if mask.sum() == 0: \n",
    "            continue\n",
    "        conf = y_prob[mask].mean()\n",
    "        acc  = y_true[mask].mean()\n",
    "        ece += (mask.mean()) * abs(acc - conf)\n",
    "    return ece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92920e65",
   "metadata": {},
   "source": [
    "## 4) Feature Engineering (Player-level)\n",
    "\n",
    "We compute **pre-game** features only (no leakage):\n",
    "- **Temporal Elo with decay** (global, by side, by role)\n",
    "- **Side & role** rolling performance\n",
    "- **Breaks/freshness** via id gaps\n",
    "- **Role-specific history** (experience and WR on that role)\n",
    "- **Same-team synergy** & **Enemy familiarity**\n",
    "- **Streaks** (win/loss)\n",
    "- **Meta eras** (bucket by id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a43a6",
   "metadata": {},
   "source": [
    "### 4.1 Meta eras & gap features\n",
    "- `meta_period`: bucket `id` into eras to capture rule changes.\n",
    "- `gap_id` per player → `gap_id_clipped` (bounded) and `long_break_flag`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ff9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta eras\n",
    "bins   = [0, 200_000, 400_000, 600_000, 800_000, 1_000_000_000]\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "df['meta_period'] = pd.cut(df['id'], bins=bins, labels=labels, include_lowest=True).astype('int8')\n",
    "\n",
    "# Gap per player (id as time proxy)\n",
    "df = df.sort_values(['player_id','id']).copy()\n",
    "df['gap_id'] = df.groupby('player_id')['id'].diff().fillna(0).astype('int64')\n",
    "df['gap_id_clipped'] = np.clip(df['gap_id'], 0, 5000).astype('int32')\n",
    "GAP_THRESH = 381  # adjust via quantiles if desired\n",
    "df['long_break_flag'] = (df['gap_id'] >= GAP_THRESH).astype('int8')\n",
    "\n",
    "# Restore global order\n",
    "df = df.sort_values('id').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62bd0e",
   "metadata": {},
   "source": [
    "### 4.2 Temporal Elo with decay\n",
    "We update Elo **after** each game. Each update is scaled by `exp(-gap/tau)` so **older inactivity** reduces update size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2843edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elos(dfin, init=1500, k=24, tau=300.0):\n",
    "    d = dfin.sort_values('id').copy()\n",
    "    elo_global, elo_side, elo_role = {}, {}, {}\n",
    "    last_seen = {}\n",
    "    outs = []\n",
    "\n",
    "    for gid, g in d.groupby('game_id', sort=False):\n",
    "        cur = g.copy()\n",
    "        cur['pre_elo']      = [elo_global.get(pid, init) for pid in cur['player_id']]\n",
    "        cur['pre_elo_side'] = [elo_side.get((pid, team), init) for pid, team in zip(cur['player_id'], cur['team'])]\n",
    "        cur['pre_elo_role'] = [elo_role.get((pid, role), init) for pid, role in zip(cur['player_id'], cur['role'])]\n",
    "\n",
    "        maf_mask  = cur['team'].eq('mafia')\n",
    "        mafia_mu  = cur.loc[maf_mask, 'pre_elo'].mean()\n",
    "        citizen_mu= cur.loc[~maf_mask, 'pre_elo'].mean()\n",
    "        exp_mafia = 1.0 / (1.0 + 10 ** ((citizen_mu - mafia_mu)/400))\n",
    "        mafia_res = int(cur.loc[maf_mask, 'team_win'].iloc[0])\n",
    "\n",
    "        for _, r in cur.iterrows():\n",
    "            pid, side, role, rid = int(r['player_id']), r['team'], r['role'], int(r['id'])\n",
    "            gap = rid - last_seen.get(pid, rid)\n",
    "            decay = float(np.exp(-max(gap,0)/float(tau)))\n",
    "            exp = exp_mafia if side=='mafia' else (1-exp_mafia)\n",
    "            act = mafia_res if side=='mafia' else (1-mafia_res)\n",
    "            delta = k * decay * (act - exp)\n",
    "\n",
    "            elo_global[pid] = elo_global.get(pid,  init) + delta\n",
    "            elo_side[(pid, side)] = elo_side.get((pid, side), init) + delta\n",
    "            elo_role[(pid, role)] = elo_role.get((pid, role), init) + delta\n",
    "            last_seen[pid] = rid\n",
    "\n",
    "        outs.append(cur[['game_id','player_id','pre_elo','pre_elo_side','pre_elo_role']])\n",
    "\n",
    "    elo_df = pd.concat(outs, ignore_index=True)\n",
    "    return d.merge(elo_df, on=['game_id','player_id'], how='left')\n",
    "\n",
    "work_players = compute_elos(df, init=1500, k=24, tau=300.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cf85f",
   "metadata": {},
   "source": [
    "### 4.3 Side-aware rolling win rates\n",
    "We track recent **team win** rates for each player **on each side** separately (mafia/citizens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114831a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_stats_side(df, windows=(5,20)):\n",
    "    d = df.sort_values(['player_id','id']).copy()\n",
    "    for side in ['mafia','citizens']:\n",
    "        mask = d['team'].eq(side)\n",
    "        d.loc[mask, f'roll5_win_rate_{side}']  = d.loc[mask].groupby('player_id')['team_win'].shift(1).rolling(windows[0], min_periods=1).mean().values\n",
    "        d.loc[mask, f'roll20_win_rate_{side}'] = d.loc[mask].groupby('player_id')['team_win'].shift(1).rolling(windows[1], min_periods=1).mean().values\n",
    "        d.loc[~mask, f'roll5_win_rate_{side}']  = 0.0\n",
    "        d.loc[~mask, f'roll20_win_rate_{side}'] = 0.0\n",
    "    return d\n",
    "\n",
    "work_players = add_rolling_stats_side(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27f27b",
   "metadata": {},
   "source": [
    "### 4.4 Role-specific history\n",
    "For each `(player, role)` compute:\n",
    "- `games_in_role` (prior count)\n",
    "- `win_rate_role_<role>_last{W}` for W in {5, 20, 50}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebe186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_role_history_stats(df, windows=(5,20,50)):\n",
    "    d = df.sort_values(['player_id','role','id']).copy()\n",
    "    out = []\n",
    "    for (pid, role), g in d.groupby(['player_id','role'], sort=False):\n",
    "        g = g.copy()\n",
    "        past = g['team_win'].shift(1)\n",
    "        g['games_in_role'] = np.arange(len(g), dtype=np.int32)\n",
    "        for w in windows:\n",
    "            g[f'win_rate_role_{role}_last{w}'] = past.rolling(w, min_periods=1).mean()\n",
    "        out.append(g)\n",
    "    return pd.concat(out, ignore_index=True).sort_values('id').reset_index(drop=True)\n",
    "\n",
    "work_players = add_role_history_stats(work_players, windows=(5,20,50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22db51",
   "metadata": {},
   "source": [
    "### 4.5 Same-team synergy\n",
    "Count prior **same-team co-plays** for all teammate pairs before this game, aggregate per team.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbfd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def add_synergy_features(df):\n",
    "    d = df.copy()\n",
    "    game_order = (d.groupby('game_id')['id'].max().sort_values().index.tolist())\n",
    "    pair_counts = {}\n",
    "    out_rows = []\n",
    "\n",
    "    for gid in game_order:\n",
    "        g = d[d['game_id'] == gid]\n",
    "        for team in ['mafia', 'citizens']:\n",
    "            players = g.loc[g['team']==team, 'player_id'].dropna().astype(int).tolist()\n",
    "            vals = [pair_counts.get((a,b,team), 0) for a,b in combinations(sorted(players), 2)] if len(players)>=2 else []\n",
    "            s_mean = float(np.mean(vals)) if vals else 0.0\n",
    "            s_max  = float(np.max(vals))  if vals else 0.0\n",
    "            out_rows.append((gid, team, s_mean, s_max))\n",
    "        # update after\n",
    "        for team in ['mafia', 'citizens']:\n",
    "            players = g.loc[g['team']==team, 'player_id'].dropna().astype(int).tolist()\n",
    "            if len(players)>=2:\n",
    "                for a,b in combinations(sorted(players), 2):\n",
    "                    pair_counts[(a,b,team)] = pair_counts.get((a,b,team),0) + 1\n",
    "\n",
    "    team_synergy = pd.DataFrame(out_rows, columns=['game_id','team','synergy_mean_team','synergy_max_team'])\n",
    "    return d.merge(team_synergy, on=['game_id','team'], how='left')\n",
    "\n",
    "work_players = add_synergy_features(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0121f3",
   "metadata": {},
   "source": [
    "### 4.6 Enemy familiarity (cross-team history)\n",
    "Count how often each player has faced each opponent **before** this game. Aggregate per team.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def add_enemy_familiarity_features(df):\n",
    "    d = df.sort_values('id').copy()\n",
    "    game_order = (d.groupby('game_id')['id'].max().sort_values().index.tolist())\n",
    "    faced_counts = {}\n",
    "    out_rows = []\n",
    "\n",
    "    for gid in game_order:\n",
    "        g = d[d['game_id'] == gid]\n",
    "        maf = g[g['team']=='mafia']['player_id'].dropna().astype(int).tolist()\n",
    "        cit = g[g['team']=='citizens']['player_id'].dropna().astype(int).tolist()\n",
    "\n",
    "        pairs_maf = [faced_counts.get(tuple(sorted([a,b])), 0) for a,b in product(maf, cit)]\n",
    "        pairs_cit = [faced_counts.get(tuple(sorted([a,b])), 0) for a,b in product(cit, maf)]\n",
    "\n",
    "        def stats(vals):\n",
    "            return (float(np.mean(vals)) if vals else 0.0,\n",
    "                    float(np.max(vals))  if vals else 0.0)\n",
    "\n",
    "        maf_mean, maf_max = stats(pairs_maf)\n",
    "        cit_mean, cit_max = stats(pairs_cit)\n",
    "\n",
    "        out_rows.append((gid,'mafia',    maf_mean, maf_max))\n",
    "        out_rows.append((gid,'citizens', cit_mean, cit_max))\n",
    "\n",
    "        for a,b in product(maf, cit):\n",
    "            key = tuple(sorted([int(a),int(b)]))\n",
    "            faced_counts[key] = faced_counts.get(key, 0) + 1\n",
    "\n",
    "    fam = pd.DataFrame(out_rows, columns=['game_id','team','enemy_fam_mean_team','enemy_fam_max_team'])\n",
    "    return d.merge(fam, on=['game_id','team'], how='left')\n",
    "\n",
    "work_players = add_enemy_familiarity_features(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbf803",
   "metadata": {},
   "source": [
    "### 4.7 Win/Loss streaks\n",
    "Compute **pre-game** consecutive win and loss streak lengths for each player.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e38950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_streak_features(df):\n",
    "    d = df.sort_values(['player_id','id']).copy()\n",
    "    win_streaks, loss_streaks = [], []\n",
    "\n",
    "    for pid, g in d.groupby('player_id', sort=False):\n",
    "        prev = g['team_win'].shift(1).values\n",
    "        w_stk = np.zeros(len(g), dtype=np.int16)\n",
    "        l_stk = np.zeros(len(g), dtype=np.int16)\n",
    "        cur_w = cur_l = 0\n",
    "        for i, v in enumerate(prev):\n",
    "            if np.isnan(v):\n",
    "                cur_w = cur_l = 0\n",
    "            else:\n",
    "                if v == 1:\n",
    "                    cur_w += 1; cur_l = 0\n",
    "                else:\n",
    "                    cur_l += 1; cur_w = 0\n",
    "            w_stk[i] = cur_w\n",
    "            l_stk[i] = cur_l\n",
    "        win_streaks.append(pd.Series(w_stk, index=g.index))\n",
    "        loss_streaks.append(pd.Series(l_stk, index=g.index))\n",
    "\n",
    "    d['win_streak']  = pd.concat(win_streaks).sort_index()\n",
    "    d['loss_streak'] = pd.concat(loss_streaks).sort_index()\n",
    "    return d.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "work_players = add_streak_features(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d7700",
   "metadata": {},
   "source": [
    "### 4.8 Games played to date (per player)\n",
    "Cumulative count of past games per player (pre-game). Useful as a general “experience” signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f835cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_games_played_feature(df):\n",
    "    d = df.sort_values(['player_id','id']).copy()\n",
    "    # number of *prior* appearances (shift to avoid leakage)\n",
    "    d['games_played'] = d.groupby('player_id').cumcount().astype('int32')\n",
    "    return d.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "work_players = add_games_played_feature(work_players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190da785",
   "metadata": {},
   "source": [
    "## 5) Team-Level Aggregation & Deltas\n",
    "\n",
    "Aggregate player features to `(game_id, team)` rows. Then create **safe deltas**: `mafia − citizens`.  \n",
    "We **never** delta the target or `meta_period`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28abd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_team_agg(work_players, add_ratios=False, ratio_eps=1e-3):\n",
    "    agg_funcs = {}\n",
    "\n",
    "    def add_agg(col, funcs):\n",
    "        if col in work_players.columns:\n",
    "            agg_funcs[col] = funcs\n",
    "\n",
    "    # Core\n",
    "    add_agg('pre_elo', ['mean','std','min','max', q25, q75])\n",
    "    add_agg('pre_elo_side', ['mean'])\n",
    "    add_agg('pre_elo_role', ['mean'])\n",
    "    add_agg('gap_id_clipped', ['mean','max'])\n",
    "    add_agg('long_break_flag', ['sum'])\n",
    "    add_agg('place', ['mean','std','min','max'])\n",
    "    add_agg('games_played', ['mean','std','min','max'])  # if present\n",
    "\n",
    "    # Optional blocks\n",
    "    add_agg('win_streak', ['mean','max'])\n",
    "    add_agg('loss_streak', ['mean','max'])\n",
    "    add_agg('synergy_mean_team', ['mean'])\n",
    "    add_agg('synergy_max_team',  ['mean'])\n",
    "    add_agg('enemy_fam_mean_team', ['mean'])\n",
    "    add_agg('enemy_fam_max_team',  ['mean'])\n",
    "    add_agg('roll5_win_rate_mafia',  ['mean'])\n",
    "    add_agg('roll20_win_rate_mafia', ['mean'])\n",
    "    add_agg('roll5_win_rate_citizens',  ['mean'])\n",
    "    add_agg('roll20_win_rate_citizens', ['mean'])\n",
    "    if 'meta_period' in work_players.columns:\n",
    "        agg_funcs['meta_period'] = ['first']\n",
    "\n",
    "    base = work_players.groupby(['game_id','team']).agg(agg_funcs)\n",
    "    base.columns = ['_'.join([str(x) for x in c if x not in (None,)]).replace('<function ','').replace('>','')\n",
    "                    for c in base.columns]\n",
    "    base = base.reset_index()\n",
    "\n",
    "    # --- NEW: meta-period normalization for Elo stats (remove era drift) ---\n",
    "    if 'meta_period_first' in base.columns:\n",
    "        elo_cols = [c for c in base.columns if c.startswith('pre_elo_')]\n",
    "        for col in elo_cols:\n",
    "            # center within meta-period\n",
    "            base[f'{col}_norm'] = base[col] - base.groupby('meta_period_first')[col].transform('mean')\n",
    "\n",
    "    # Role-specific singletons/means\n",
    "    full_idx = base.set_index(['game_id','team']).index\n",
    "    # Role-specific singletons/means\n",
    "    full_idx = base.set_index(['game_id','team']).index\n",
    "\n",
    "    def single_role_stat(role, value_col, out_name):\n",
    "        s = (work_players[work_players['role']==role]\n",
    "             .groupby(['game_id','team'])[value_col].mean()).reindex(full_idx)\n",
    "        s.name = out_name; return s\n",
    "\n",
    "    def mean_role_stat(role, value_col, out_name):\n",
    "        s = (work_players[work_players['role']==role]\n",
    "             .groupby(['game_id','team'])[value_col].mean()).reindex(full_idx)\n",
    "        s.name = out_name; return s\n",
    "\n",
    "    pieces = [\n",
    "        single_role_stat('don','pre_elo_role','don_pre_elo_role'),\n",
    "        single_role_stat('sheriff','pre_elo_role','sheriff_pre_elo_role'),\n",
    "        single_role_stat('don','place','don_place'),\n",
    "        single_role_stat('sheriff','place','sheriff_place'),\n",
    "        mean_role_stat('black','pre_elo_role','black_mean_pre_elo_role'),\n",
    "        mean_role_stat('red','pre_elo_role','red_mean_pre_elo_role'),\n",
    "        single_role_stat('don','games_in_role','don_games_in_role'),\n",
    "        single_role_stat('sheriff','games_in_role','sheriff_games_in_role'),\n",
    "        mean_role_stat('black','games_in_role','black_mean_games_in_role'),\n",
    "        mean_role_stat('red','games_in_role','red_mean_games_in_role'),\n",
    "        single_role_stat('don','win_rate_role_don_last20','don_wr20'),\n",
    "        single_role_stat('sheriff','win_rate_role_sheriff_last20','sheriff_wr20'),\n",
    "        mean_role_stat('black','win_rate_role_black_last20','black_mean_wr20'),\n",
    "        mean_role_stat('red','win_rate_role_red_last20','red_mean_wr20'),\n",
    "    ]\n",
    "    role_feats = pd.concat(pieces, axis=1).reset_index()\n",
    "    team_agg = base.merge(role_feats, on=['game_id','team'], how='left')\n",
    "\n",
    "    # Label & time proxy\n",
    "    labels  = work_players.groupby(['game_id','team'])['team_win'].max().rename('team_win_team')\n",
    "    gmaxid  = work_players.groupby('game_id')['id'].max().rename('game_max_id')\n",
    "    team_agg = team_agg.merge(labels, on=['game_id','team']).merge(gmaxid, on='game_id')\n",
    "\n",
    "    # Safe deltas / ratios\n",
    "    wide = team_agg.pivot(index='game_id', columns='team')\n",
    "    wide.columns = [f\"{a}__{b}\" for a,b in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "\n",
    "    def side_cols(side): \n",
    "        return [c for c in wide.columns if c.endswith(f\"__{side}\") and c!='game_id']\n",
    "    maf_cols = side_cols('mafia')\n",
    "\n",
    "    delta = pd.DataFrame({'game_id': wide['game_id']})\n",
    "    skip_prefixes = ('team_win_team','meta_period')\n",
    "    for mcol in maf_cols:\n",
    "        base_name = mcol[:-len(\"__mafia\")]\n",
    "        if base_name.startswith(skip_prefixes): \n",
    "            continue\n",
    "        ccol = base_name + \"__citizens\"\n",
    "        if ccol in wide.columns:\n",
    "            delta[base_name + \"__delta_maf_minus_cit\"] = wide[mcol] - wide[ccol]\n",
    "            if add_ratios:\n",
    "                delta[base_name + \"__ratio_maf_over_cit\"] = (wide[mcol] + ratio_eps) / (wide[ccol] + ratio_eps)\n",
    "\n",
    "    team_tall = team_agg.merge(delta, on='game_id', how='left')\n",
    "\n",
    "    # --- NEW: a few safe interactions (helps tree models separate regimes) ---\n",
    "    def safe_mul(a, b): \n",
    "        return (team_tall.get(a) if a in team_tall else 0) * (team_tall.get(b) if b in team_tall else 0)\n",
    "\n",
    "    def safe_diff(a, b): \n",
    "        return (team_tall.get(a) if a in team_tall else 0) - (team_tall.get(b) if b in team_tall else 0)\n",
    "\n",
    "    # Names used below exist after delta creation; if any is missing in your run, it's treated as 0\n",
    "    team_tall['elo_synergy_product'] = safe_mul('pre_elo_mean__delta_maf_minus_cit',\n",
    "                                                'synergy_mean_team_mean__delta_maf_minus_cit')\n",
    "    team_tall['elo_enemy_gap']       = safe_diff('pre_elo_mean__delta_maf_minus_cit',\n",
    "                                                'enemy_fam_mean_team_mean__delta_maf_minus_cit')\n",
    "    team_tall['elo_streak_mix']      = safe_mul('pre_elo_mean__delta_maf_minus_cit',\n",
    "                                                'win_streak_mean__delta_maf_minus_cit')\n",
    "\n",
    "    return team_tall\n",
    "\n",
    "team_tall = build_team_agg(work_players, add_ratios=False)  # ratios often redundant\n",
    "sanity_assert_two_rows_per_game(team_tall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490c82f",
   "metadata": {},
   "source": [
    "## 6) Feature Selection\n",
    "\n",
    "We include **team-only** features and **deltas**. We **exclude** label-like columns.\n",
    "Then we create **time-aware** train/cal/test splits (70/15/15 by `game_max_id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f392194",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_only = [c for c in team_tall.columns if c.startswith((\n",
    "    'pre_elo_', 'gap_id_clipped_', 'long_break_flag_', 'place_',\n",
    "    'win_streak_', 'loss_streak_', 'synergy_mean_team_', 'synergy_max_team_',\n",
    "    'enemy_fam_', 'games_played_', \n",
    "    'don_pre_elo_role', 'sheriff_pre_elo_role', 'black_mean_pre_elo_role', 'red_mean_pre_elo_role',\n",
    "    'don_games_in_role', 'sheriff_games_in_role', 'black_mean_games_in_role', 'red_mean_games_in_role',\n",
    "    'don_wr20', 'sheriff_wr20', 'black_mean_wr20', 'red_mean_wr20',\n",
    "    'meta_period_first'\n",
    "))]\n",
    "delta_feats = [c for c in team_tall.columns if c.endswith('__delta_maf_minus_cit')]\n",
    "\n",
    "# NEW: explicitly add our interactions and meta-normalized Elo columns\n",
    "extra_feats = [c for c in ['elo_synergy_product','elo_enemy_gap','elo_streak_mix']\n",
    "               if c in team_tall.columns]\n",
    "meta_norm_feats = [c for c in team_tall.columns if c.endswith('_norm')]\n",
    "\n",
    "forbidden_tokens = {'team_win','team_win_team'}\n",
    "USED_FEATS = [c for c in sorted(set(team_only + delta_feats + extra_feats + meta_norm_feats))\n",
    "              if not any(tok in c for tok in forbidden_tokens)]\n",
    "\n",
    "X = team_tall[USED_FEATS].fillna(0)\n",
    "y = team_tall['team_win_team'].astype(int).values\n",
    "groups = team_tall['game_id'].values\n",
    "time_key = team_tall['game_max_id'].values\n",
    "\n",
    "q70, q85 = np.quantile(time_key, [0.70, 0.85])\n",
    "train_mask = time_key <= q85\n",
    "cal_mask   = (time_key > q70) & (time_key <= q85)\n",
    "test_mask  = time_key > q85\n",
    "\n",
    "print(\"Shapes | X:\", X.shape, \"| y:\", y.shape)\n",
    "print(\"Split sizes | train:\", train_mask.sum(), \"cal:\", cal_mask.sum(), \"test:\", test_mask.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11660c7",
   "metadata": {},
   "source": [
    "## 7) Model — LightGBM (calibrated)\n",
    "Train on train, calibrate on cal (sigmoid), evaluate on holdout (last 15%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ddd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=127,\n",
    "    min_data_in_leaf=60,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.5,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Optional inner early stopping on last 10% of train\n",
    "tr_time = time_key[train_mask]; q90 = np.quantile(tr_time, 0.90)\n",
    "inner_tr = train_mask & (time_key <= q90)\n",
    "inner_va = train_mask & (time_key >  q90)\n",
    "\n",
    "lgb = LGBMClassifier(**params)\n",
    "lgb.fit(\n",
    "    X[inner_tr], y[inner_tr],\n",
    "    eval_set=[(X[inner_va], y[inner_va])],\n",
    "    eval_metric='logloss',\n",
    "    callbacks=[early_stopping(100), log_evaluation(0)]\n",
    ")\n",
    "\n",
    "# Freeze the already-fitted estimator (workaround for prefit deprecation)\n",
    "frozen = clone(lgb)\n",
    "frozen.__dict__.update(lgb.__dict__)\n",
    "\n",
    "# Sigmoid (Platt) calibration on the calibration slice\n",
    "calibrated = CalibratedClassifierCV(frozen, cv=\"prefit\", method=\"sigmoid\")\n",
    "calibrated.fit(X[cal_mask], y[cal_mask])\n",
    "\n",
    "p_test = calibrated.predict_proba(X[test_mask])[:,1]\n",
    "\n",
    "print(\"Holdout (last 15%)\")\n",
    "print(\"LogLoss:\", log_loss(y[test_mask], p_test))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y[test_mask], p_test))\n",
    "print(\"Brier  :\", brier_score_loss(y[test_mask], p_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199751e",
   "metadata": {},
   "source": [
    "## 8) Calibration Diagnostics\n",
    "\n",
    "Reliability curve and **ECE** (Expected Calibration Error) on holdout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ece = expected_calibration_error(y[test_mask], p_test, n_bins=10)\n",
    "print(f\"ECE (10 bins): {ece:.4f}\")\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y[test_mask], p_test, n_bins=10)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(prob_pred, prob_true, 'o-')\n",
    "plt.plot([0,1],[0,1],'--', alpha=0.6)\n",
    "plt.xlabel('Predicted probability'); plt.ylabel('Observed frequency')\n",
    "plt.title('Reliability curve (holdout)')\n",
    "plt.grid(alpha=0.2); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4020ff5",
   "metadata": {},
   "source": [
    "## 9) (Optional) CatBoost + Calibration + Blend\n",
    "\n",
    "CatBoost often complements tree ensembles. We calibrate its probabilities and optionally blend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8022c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CATBOOST_AVAILABLE:\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_cal,   y_cal   = X[cal_mask],   y[cal_mask]\n",
    "    X_hold,  y_hold  = X[test_mask],  y[test_mask]\n",
    "\n",
    "    cat_features = []\n",
    "    if 'meta_period_first' in X.columns:\n",
    "        cat_features.append(X.columns.get_loc('meta_period_first'))\n",
    "\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features or None)\n",
    "    cal_pool   = Pool(X_cal,   y_cal,   cat_features=cat_features or None)\n",
    "    test_pool  = Pool(X_hold,  y_hold,  cat_features=cat_features or None)\n",
    "\n",
    "    cat = CatBoostClassifier(\n",
    "        iterations=2500,\n",
    "        learning_rate=0.02,\n",
    "        depth=8,\n",
    "        l2_leaf_reg=3.0,\n",
    "        random_seed=42,\n",
    "        eval_metric='Logloss',\n",
    "        loss_function='Logloss',\n",
    "        class_weights=[1.0, 2.3],\n",
    "        use_best_model=True,\n",
    "        verbose=200\n",
    "    )\n",
    "    cat.fit(train_pool, eval_set=cal_pool)\n",
    "\n",
    "   # Sigmoid (baseline)\n",
    "    cat_cal_sig = CalibratedClassifierCV(cat, cv='prefit', method='sigmoid').fit(X[cal_mask], y[cal_mask])\n",
    "    p_cat_sig = cat_cal_sig.predict_proba(X[test_mask])[:,1]\n",
    "\n",
    "    # Isotonic (may help CatBoost LogLoss if enough cal data)\n",
    "    cat_cal_iso = CalibratedClassifierCV(cat, cv='prefit', method='isotonic').fit(X[cal_mask], y[cal_mask])\n",
    "    p_cat_iso = cat_cal_iso.predict_proba(X[test_mask])[:,1]\n",
    "\n",
    "    # Pick the better calibrated CatBoost for blending (by LogLoss)\n",
    "    ll_sig = log_loss(y[test_mask], p_cat_sig)\n",
    "    ll_iso = log_loss(y[test_mask], p_cat_iso)\n",
    "    if ll_iso < ll_sig:\n",
    "        p_cat = p_cat_iso\n",
    "        print(\"Using CatBoost isotonic calibration (better LogLoss).\")\n",
    "    else:\n",
    "        p_cat = p_cat_sig\n",
    "        print(\"Using CatBoost sigmoid calibration (better or equal LogLoss).\")\n",
    "\n",
    "    print(\"\\nCatBoost (calibrated) — Holdout\")\n",
    "    print(\"LogLoss:\", log_loss(y[test_mask], p_cat))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y[test_mask], p_cat))\n",
    "    print(\"Brier  :\", brier_score_loss(y[test_mask], p_cat))\n",
    "\n",
    "    # blend\n",
    "    w_lgbm = 0.6\n",
    "    w_cat  = 0.4\n",
    "    p_blend = w_lgbm * p_test + w_cat * p_cat\n",
    "    print(f\"\\nBlend {w_lgbm:.1f}·LGBM + {w_cat:.1f}·Cat — Holdout\")\n",
    "\n",
    "    print(\"LogLoss:\", log_loss(y[test_mask], p_blend))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y[test_mask], p_blend))\n",
    "    print(\"Brier  :\", brier_score_loss(y[test_mask], p_blend))\n",
    "else:\n",
    "    print(\"CatBoost not installed — skipping optional block.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d692dc",
   "metadata": {},
   "source": [
    "### (Optional) XGBoost third learner\n",
    "A slightly different tree engine sometimes helps the ensemble by a small margin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9899ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=1200, learning_rate=0.015,\n",
    "        max_depth=7, subsample=0.9, colsample_bytree=0.9,\n",
    "        reg_lambda=1.0, reg_alpha=0.5,\n",
    "        objective='binary:logistic', eval_metric='logloss',\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    p_xgb = xgb.predict_proba(X_hold)[:,1]\n",
    "\n",
    "    # quick 3-way blend (adjust weights after checking LogLoss)\n",
    "    p_blend3 = 0.5*p_lgbm_final + 0.3*(p_cat if p_cat is not None else 0) + 0.2*p_xgb\n",
    "    print(\"\\n3-way Blend — Holdout\")\n",
    "    print(\"LogLoss:\", log_loss(y_hold, p_blend3))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_hold, p_blend3))\n",
    "    print(\"Brier  :\", brier_score_loss(y_hold, p_blend3))\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not available or failed to fit:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f72fcf",
   "metadata": {},
   "source": [
    "## 10) Summary & Next Steps\n",
    "\n",
    "- Leak-free **pre-game** predictor, calibrated probabilities.\n",
    "- Strong signals: Elo deltas/means, synergy, streaks, role history, freshness.\n",
    "- Try: different **Elo decay** `tau`, **role-pair synergy**, **CatBoost blending**, or small **Optuna** tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42deffa",
   "metadata": {},
   "source": [
    "## 11) Quick Hyper-parameter & Blend Tuning (Optuna)\n",
    "\n",
    "We tune a handful of high-impact LightGBM parameters and the blend weights\n",
    "between LGBM and CatBoost. We keep a fixed learning rate and estimators for stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.base import clone\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_cal,   y_cal   = X[cal_mask],   y[cal_mask]\n",
    "X_hold,  y_hold  = X[test_mask],  y[test_mask]\n",
    "\n",
    "def train_lgbm_with_params(params_dict):\n",
    "    lgb = LGBMClassifier(\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=int(params_dict['num_leaves']),\n",
    "        min_data_in_leaf=int(params_dict['min_data_in_leaf']),\n",
    "        subsample=float(params_dict['subsample']),\n",
    "        colsample_bytree=float(params_dict['colsample_bytree']),\n",
    "        reg_lambda=float(params_dict['reg_lambda']),\n",
    "        reg_alpha=float(params_dict['reg_alpha']),\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    # inner early-stop on last 10% of train time\n",
    "    tr_t = time_key[train_mask]; q90 = np.quantile(tr_t, 0.90)\n",
    "    inner_tr = (time_key <= q90) & train_mask\n",
    "    inner_va = (time_key >  q90) & train_mask\n",
    "\n",
    "    lgb.fit(\n",
    "        X[inner_tr], y[inner_tr],\n",
    "        eval_set=[(X[inner_va], y[inner_va])],\n",
    "        eval_metric='logloss',\n",
    "        callbacks=[early_stopping(100), log_evaluation(0)]\n",
    "    )\n",
    "    # Calibrate on cal split (sigmoid)\n",
    "    fr = clone(lgb); fr.__dict__.update(lgb.__dict__)\n",
    "    cal = CalibratedClassifierCV(fr, cv='prefit', method='sigmoid')\n",
    "    cal.fit(X_cal, y_cal)\n",
    "    p = cal.predict_proba(X_hold)[:,1]\n",
    "    return p, lgb\n",
    "\n",
    "# Cache CatBoost predictions from your earlier section (if available)\n",
    "# If not run yet, train a small Cat here:\n",
    "if CATBOOST_AVAILABLE:\n",
    "    try:\n",
    "        p_cat  # exists\n",
    "    except NameError:\n",
    "        train_pool = Pool(X_train, y_train)\n",
    "        cal_pool   = Pool(X_cal,   y_cal)\n",
    "        cat = CatBoostClassifier(\n",
    "            iterations=2000, learning_rate=0.02, depth=8,\n",
    "            l2_leaf_reg=3.0, random_seed=42,\n",
    "            eval_metric='Logloss', loss_function='Logloss',\n",
    "            auto_class_weights='Balanced',\n",
    "            use_best_model=True, verbose=False\n",
    "        )\n",
    "        cat.fit(train_pool, eval_set=cal_pool, verbose=False)\n",
    "        cat_cal = CalibratedClassifierCV(cat, cv='prefit', method='sigmoid').fit(X_cal, y_cal)\n",
    "        p_cat = cat_cal.predict_proba(X_hold)[:,1]\n",
    "else:\n",
    "    p_cat = None  # tuning will ignore blend weight\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    # LGBM params to tune (narrow, safe ranges)\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 63, 159),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 40, 140),\n",
    "        'subsample': trial.suggest_float('subsample', 0.70, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.75, 1.00),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 3.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 3.0),\n",
    "    }\n",
    "    p_lgbm, _ = train_lgbm_with_params(params)\n",
    "\n",
    "    if p_cat is not None:\n",
    "        w_lgbm = trial.suggest_float('w_lgbm', 0.4, 0.9)\n",
    "        w_cat  = 1.0 - w_lgbm\n",
    "        p_blend = w_lgbm * p_lgbm + w_cat * p_cat\n",
    "        loss = log_loss(y_hold, p_blend)\n",
    "    else:\n",
    "        loss = log_loss(y_hold, p_lgbm)\n",
    "\n",
    "    # report auxiliary metrics\n",
    "    trial.set_user_attr('AUC', roc_auc_score(y_hold, p_lgbm if p_cat is None else p_blend))\n",
    "    trial.set_user_attr('Brier', brier_score_loss(y_hold, p_lgbm if p_cat is None else p_blend))\n",
    "    return loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "print(\"Best LogLoss:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best AUC (attr):\", study.best_trial.user_attrs.get('AUC'))\n",
    "print(\"Best Brier (attr):\", study.best_trial.user_attrs.get('Brier'))\n",
    "\n",
    "# Refit final LGBM with best params and produce final blended predictions\n",
    "best_params = study.best_params.copy()\n",
    "w_lgbm = best_params.pop('w_lgbm', 1.0)  # present only if CatBoost available\n",
    "p_lgbm_final, lgb_final = train_lgbm_with_params(best_params)\n",
    "\n",
    "if p_cat is not None:\n",
    "    w_cat = 1.0 - w_lgbm\n",
    "    p_final = w_lgbm * p_lgbm_final + w_cat * p_cat\n",
    "    print(f\"\\nFinal blend weights: LGBM={w_lgbm:.2f}, Cat={w_cat:.2f}\")\n",
    "else:\n",
    "    p_final = p_lgbm_final\n",
    "\n",
    "print(\"\\nTuned — Holdout\")\n",
    "print(\"LogLoss:\", log_loss(y_hold, p_final))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_hold, p_final))\n",
    "print(\"Brier  :\", brier_score_loss(y_hold, p_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531dae28",
   "metadata": {},
   "source": [
    "### (Optional) Micro-sweep of Elo decay τ\n",
    "Quick check around τ=400 to confirm the best setting on this dataset split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taus = [300, 350, 400, 450, 500]\n",
    "# results = []\n",
    "# for t in taus:\n",
    "#     # --- rebuild features with this tau ---\n",
    "#     wp = compute_elos(df, init=1500, k=24, tau=float(t))\n",
    "#     wp = add_rolling_stats_side(wp)\n",
    "#     wp = add_role_history_stats(wp, windows=(5,20,50))\n",
    "#     wp = add_synergy_features(wp)\n",
    "#     wp = add_enemy_familiarity_features(wp)\n",
    "#     wp = add_streak_features(wp)\n",
    "#     wp = add_games_played_feature(wp)\n",
    "#     team_tall = build_team_agg(wp, add_ratios=False)\n",
    "\n",
    "#     # feature selection (same logic as main flow)\n",
    "#     team_only = [c for c in team_tall.columns if c.startswith((\n",
    "#         'pre_elo_', 'gap_id_clipped_', 'long_break_flag_', 'place_',\n",
    "#         'win_streak_', 'loss_streak_', 'synergy_mean_team_', 'synergy_max_team_',\n",
    "#         'enemy_fam_', 'games_played_', \n",
    "#         'don_pre_elo_role', 'sheriff_pre_elo_role', 'black_mean_pre_elo_role', 'red_mean_pre_elo_role',\n",
    "#         'don_games_in_role', 'sheriff_games_in_role', 'black_mean_games_in_role', 'red_mean_games_in_role',\n",
    "#         'don_wr20', 'sheriff_wr20', 'black_mean_wr20', 'red_mean_wr20',\n",
    "#         'meta_period_first'\n",
    "#     ))]\n",
    "#     delta_feats = [c for c in team_tall.columns if c.endswith('__delta_maf_minus_cit')]\n",
    "#     extra_feats = [c for c in ['elo_synergy_product','elo_enemy_gap','elo_streak_mix'] if c in team_tall.columns]\n",
    "#     meta_norm_feats = [c for c in team_tall.columns if c.endswith('_norm')]\n",
    "\n",
    "#     forbidden_tokens = {'team_win','team_win_team'}\n",
    "#     USED_FEATS = [c for c in sorted(set(team_only + delta_feats + extra_feats + meta_norm_feats))\n",
    "#                   if not any(tok in c for tok in forbidden_tokens)]\n",
    "\n",
    "#     X = team_tall[USED_FEATS].fillna(0).values\n",
    "#     y = team_tall['team_win_team'].astype(int).values\n",
    "#     time_key = team_tall['game_max_id'].values\n",
    "\n",
    "#     # time-aware split\n",
    "#     q70, q85 = np.quantile(time_key, [0.70, 0.85])\n",
    "#     train_mask = time_key <= q85\n",
    "#     cal_mask   = (time_key > q70) & (time_key <= q85)\n",
    "#     test_mask  = time_key > q85\n",
    "\n",
    "#     # inner early-stop split (last 10% of train)\n",
    "#     tr_time = time_key[train_mask]\n",
    "#     q90 = np.quantile(tr_time, 0.90)\n",
    "#     inner_tr = train_mask & (time_key <= q90)\n",
    "#     inner_va = train_mask & (time_key >  q90)\n",
    "\n",
    "#     # model\n",
    "#     lgb = LGBMClassifier(\n",
    "#         n_estimators=1500, learning_rate=0.01,\n",
    "#         num_leaves=127, min_data_in_leaf=60,\n",
    "#         subsample=0.9, colsample_bytree=0.9,\n",
    "#         reg_lambda=1.0, reg_alpha=0.5,\n",
    "#         class_weight='balanced', random_state=42, n_jobs=-1\n",
    "#     )\n",
    "#     lgb.fit(\n",
    "#         X[inner_tr], y[inner_tr],\n",
    "#         eval_set=[(X[inner_va], y[inner_va])],\n",
    "#         eval_metric='logloss',\n",
    "#         callbacks=[early_stopping(100), log_evaluation(0)]\n",
    "#     )\n",
    "\n",
    "#     # calibration on cal split — use the *fitted* estimator directly\n",
    "#     cal = CalibratedClassifierCV(lgb, cv='prefit', method='sigmoid').fit(X[cal_mask], y[cal_mask])\n",
    "#     p = cal.predict_proba(X[test_mask])[:,1]\n",
    "\n",
    "#     auc = roc_auc_score(y[test_mask], p)\n",
    "#     ll  = log_loss(y[test_mask], p)\n",
    "#     print(f\"tau={t}  AUC={auc:.4f}  LogLoss={ll:.4f}\")\n",
    "#     results.append((t, auc, ll))\n",
    "\n",
    "# # pick best by LogLoss\n",
    "# best = min(results, key=lambda x: x[2])\n",
    "# print(\"Best (by LogLoss):\", best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
