{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527eb4c4",
   "metadata": {},
   "source": [
    "# Final Model Pipeline — LGBM + CatBoost (τ=300, calibrated, blended)\n",
    "\n",
    "This notebook is a clean, linear rebuild:\n",
    "- Fixed Elo decay **τ = 300**\n",
    "- Stable time split with **holdout = last 15%**\n",
    "- Consistent masks: `fit_mask` (train base), `cal_mask` (calibration), `test_mask` (final)\n",
    "- LightGBM tuned params from your search; CatBoost baseline\n",
    "- Platt sigmoid calibration (`cv='prefit'`)\n",
    "- Blended predictions with your tuned weight\n",
    "- Compact metrics utilities, optional threshold & importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e6536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Modeling & metrics\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb8385cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set. SEED=42, TAU=300\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS / CONFIG ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score, log_loss, brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Lock Elo decay tau (as requested)\n",
    "TAU = 300\n",
    "\n",
    "def seed_everywhere(seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everywhere()\n",
    "print(f\"Config set. SEED={SEED}, TAU={TAU}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f35289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ROW INDEXING HELPER (works for DataFrame/Series/ndarray) ===\n",
    "def rows(A, indexer):\n",
    "    \"\"\"Row-subset A by indexer (positional ints or boolean mask), for pandas or numpy.\"\"\"\n",
    "    if hasattr(A, \"iloc\"):\n",
    "        return A.iloc[indexer]\n",
    "    return A[indexer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d5f8c",
   "metadata": {},
   "source": [
    "### DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7c2ea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (802820, 21) columns: 21\n"
     ]
    }
   ],
   "source": [
    "DATA_CSV = Path(\"cleaned/mafia_clean.csv\")   # put the CSV next to this notebook or provide an absolute path\n",
    "OUT_DIR  = Path(\"cleaned\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print(\"Loaded:\", df.shape, \"columns:\", len(df.columns))\n",
    "assert {'id','game_id','player_id','role','team','game_points','team_win'}.issubset(df.columns), \\\n",
    "    \"Missing required columns in the cleaned dataset.\"\n",
    "\n",
    "# Basic coercions\n",
    "df['id'] = pd.to_numeric(df['id'], errors='coerce').astype('int64')\n",
    "df['game_id'] = pd.to_numeric(df['game_id'], errors='coerce').astype('int64')\n",
    "df['player_id'] = pd.to_numeric(df['player_id'], errors='coerce').astype('int64')\n",
    "df['team_win'] = pd.to_numeric(df['team_win'], errors='coerce').astype('int8')\n",
    "df['team'] = df['team'].astype('category')\n",
    "df['role'] = df['role'].astype('category')\n",
    "\n",
    "# Seat/position optional column name normalization (if present)\n",
    "if 'place' in df.columns:\n",
    "    df['place'] = pd.to_numeric(df['place'], errors='coerce').fillna(0).astype('int16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c7d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta eras\n",
    "bins   = [0, 200_000, 400_000, 600_000, 800_000, 1_000_000_000]\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "df['meta_period'] = pd.cut(df['id'], bins=bins, labels=labels, include_lowest=True).astype('int8')\n",
    "\n",
    "# Gap per player (id as time proxy)\n",
    "df = df.sort_values(['player_id','id']).copy()\n",
    "df['gap_id'] = df.groupby('player_id')['id'].diff().fillna(0).astype('int64')\n",
    "df['gap_id_clipped'] = np.clip(df['gap_id'], 0, 5000).astype('int32')\n",
    "GAP_THRESH = 381  # adjust via quantiles if desired\n",
    "df['long_break_flag'] = (df['gap_id'] >= GAP_THRESH).astype('int8')\n",
    "\n",
    "# Restore global order\n",
    "df = df.sort_values('id').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elos(dfin, init=1500, k=24, tau=300.0):\n",
    "    d = dfin.sort_values('id').copy()\n",
    "    elo_global, elo_side, elo_role = {}, {}, {}\n",
    "    last_seen = {}\n",
    "    outs = []\n",
    "\n",
    "    for gid, g in d.groupby('game_id', sort=False):\n",
    "        cur = g.copy()\n",
    "        cur['pre_elo']      = [elo_global.get(pid, init) for pid in cur['player_id']]\n",
    "        cur['pre_elo_side'] = [elo_side.get((pid, team), init) for pid, team in zip(cur['player_id'], cur['team'])]\n",
    "        cur['pre_elo_role'] = [elo_role.get((pid, role), init) for pid, role in zip(cur['player_id'], cur['role'])]\n",
    "\n",
    "        maf_mask  = cur['team'].eq('mafia')\n",
    "        mafia_mu  = cur.loc[maf_mask, 'pre_elo'].mean()\n",
    "        citizen_mu= cur.loc[~maf_mask, 'pre_elo'].mean()\n",
    "        exp_mafia = 1.0 / (1.0 + 10 ** ((citizen_mu - mafia_mu)/400))\n",
    "        mafia_res = int(cur.loc[maf_mask, 'team_win'].iloc[0])\n",
    "\n",
    "        for _, r in cur.iterrows():\n",
    "            pid, side, role, rid = int(r['player_id']), r['team'], r['role'], int(r['id'])\n",
    "            gap = rid - last_seen.get(pid, rid)\n",
    "            decay = float(np.exp(-max(gap,0)/float(tau)))\n",
    "            exp = exp_mafia if side=='mafia' else (1-exp_mafia)\n",
    "            act = mafia_res if side=='mafia' else (1-mafia_res)\n",
    "            delta = k * decay * (act - exp)\n",
    "\n",
    "            elo_global[pid] = elo_global.get(pid,  init) + delta\n",
    "            elo_side[(pid, side)] = elo_side.get((pid, side), init) + delta\n",
    "            elo_role[(pid, role)] = elo_role.get((pid, role), init) + delta\n",
    "            last_seen[pid] = rid\n",
    "\n",
    "        outs.append(cur[['game_id','player_id','pre_elo','pre_elo_side','pre_elo_role']])\n",
    "\n",
    "    elo_df = pd.concat(outs, ignore_index=True)\n",
    "    return d.merge(elo_df, on=['game_id','player_id'], how='left')\n",
    "\n",
    "work_players = compute_elos(df, init=1500, k=24, tau=300.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ec5ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_stats_side(df, windows=(5,20)):\n",
    "    d = df.sort_values(['player_id','id']).copy()\n",
    "    for side in ['mafia','citizens']:\n",
    "        mask = d['team'].eq(side)\n",
    "        d.loc[mask, f'roll5_win_rate_{side}']  = d.loc[mask].groupby('player_id')['team_win'].shift(1).rolling(windows[0], min_periods=1).mean().values\n",
    "        d.loc[mask, f'roll20_win_rate_{side}'] = d.loc[mask].groupby('player_id')['team_win'].shift(1).rolling(windows[1], min_periods=1).mean().values\n",
    "        d.loc[~mask, f'roll5_win_rate_{side}']  = 0.0\n",
    "        d.loc[~mask, f'roll20_win_rate_{side}'] = 0.0\n",
    "    return d\n",
    "\n",
    "work_players = add_rolling_stats_side(work_players)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0637034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_role_history_stats(df, windows=(5,20,50)):\n",
    "    d = df.sort_values(['player_id','role','id']).copy()\n",
    "    out = []\n",
    "    for (pid, role), g in d.groupby(['player_id','role'], sort=False):\n",
    "        g = g.copy()\n",
    "        past = g['team_win'].shift(1)\n",
    "        g['games_in_role'] = np.arange(len(g), dtype=np.int32)\n",
    "        for w in windows:\n",
    "            g[f'win_rate_role_{role}_last{w}'] = past.rolling(w, min_periods=1).mean()\n",
    "        out.append(g)\n",
    "    return pd.concat(out, ignore_index=True).sort_values('id').reset_index(drop=True)\n",
    "\n",
    "work_players = add_role_history_stats(work_players, windows=(5,20,50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c1fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def add_synergy_features(df):\n",
    "    d = df.copy()\n",
    "    game_order = (d.groupby('game_id')['id'].max().sort_values().index.tolist())\n",
    "    pair_counts = {}\n",
    "    out_rows = []\n",
    "\n",
    "    for gid in game_order:\n",
    "        g = d[d['game_id'] == gid]\n",
    "        for team in ['mafia', 'citizens']:\n",
    "            players = g.loc[g['team']==team, 'player_id'].dropna().astype(int).tolist()\n",
    "            vals = [pair_counts.get((a,b,team), 0) for a,b in combinations(sorted(players), 2)] if len(players)>=2 else []\n",
    "            s_mean = float(np.mean(vals)) if vals else 0.0\n",
    "            s_max  = float(np.max(vals))  if vals else 0.0\n",
    "            out_rows.append((gid, team, s_mean, s_max))\n",
    "        # update after\n",
    "        for team in ['mafia', 'citizens']:\n",
    "            players = g.loc[g['team']==team, 'player_id'].dropna().astype(int).tolist()\n",
    "            if len(players)>=2:\n",
    "                for a,b in combinations(sorted(players), 2):\n",
    "                    pair_counts[(a,b,team)] = pair_counts.get((a,b,team),0) + 1\n",
    "\n",
    "    team_synergy = pd.DataFrame(out_rows, columns=['game_id','team','synergy_mean_team','synergy_max_team'])\n",
    "    return d.merge(team_synergy, on=['game_id','team'], how='left')\n",
    "\n",
    "work_players = add_synergy_features(work_players)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd214310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def add_enemy_familiarity_features(df):\n",
    "    d = df.sort_values('id').copy()\n",
    "    game_order = (d.groupby('game_id')['id'].max().sort_values().index.tolist())\n",
    "    faced_counts = {}\n",
    "    out_rows = []\n",
    "\n",
    "    for gid in game_order:\n",
    "        g = d[d['game_id'] == gid]\n",
    "        maf = g[g['team']=='mafia']['player_id'].dropna().astype(int).tolist()\n",
    "        cit = g[g['team']=='citizens']['player_id'].dropna().astype(int).tolist()\n",
    "\n",
    "        pairs_maf = [faced_counts.get(tuple(sorted([a,b])), 0) for a,b in product(maf, cit)]\n",
    "        pairs_cit = [faced_counts.get(tuple(sorted([a,b])), 0) for a,b in product(cit, maf)]\n",
    "\n",
    "        def stats(vals):\n",
    "            return (float(np.mean(vals)) if vals else 0.0,\n",
    "                    float(np.max(vals))  if vals else 0.0)\n",
    "\n",
    "        maf_mean, maf_max = stats(pairs_maf)\n",
    "        cit_mean, cit_max = stats(pairs_cit)\n",
    "\n",
    "        out_rows.append((gid,'mafia',    maf_mean, maf_max))\n",
    "        out_rows.append((gid,'citizens', cit_mean, cit_max))\n",
    "\n",
    "        for a,b in product(maf, cit):\n",
    "            key = tuple(sorted([int(a),int(b)]))\n",
    "            faced_counts[key] = faced_counts.get(key, 0) + 1\n",
    "\n",
    "    fam = pd.DataFrame(out_rows, columns=['game_id','team','enemy_fam_mean_team','enemy_fam_max_team'])\n",
    "    return d.merge(fam, on=['game_id','team'], how='left')\n",
    "\n",
    "work_players = add_enemy_familiarity_features(work_players)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69d49caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_streak_features(df):\n",
    "    d = df.sort_values(['player_id','id']).copy()\n",
    "    win_streaks, loss_streaks = [], []\n",
    "\n",
    "    for pid, g in d.groupby('player_id', sort=False):\n",
    "        prev = g['team_win'].shift(1).values\n",
    "        w_stk = np.zeros(len(g), dtype=np.int16)\n",
    "        l_stk = np.zeros(len(g), dtype=np.int16)\n",
    "        cur_w = cur_l = 0\n",
    "        for i, v in enumerate(prev):\n",
    "            if np.isnan(v):\n",
    "                cur_w = cur_l = 0\n",
    "            else:\n",
    "                if v == 1:\n",
    "                    cur_w += 1; cur_l = 0\n",
    "                else:\n",
    "                    cur_l += 1; cur_w = 0\n",
    "            w_stk[i] = cur_w\n",
    "            l_stk[i] = cur_l\n",
    "        win_streaks.append(pd.Series(w_stk, index=g.index))\n",
    "        loss_streaks.append(pd.Series(l_stk, index=g.index))\n",
    "\n",
    "    d['win_streak']  = pd.concat(win_streaks).sort_index()\n",
    "    d['loss_streak'] = pd.concat(loss_streaks).sort_index()\n",
    "    return d.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "work_players = add_streak_features(work_players)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcbd5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_games_played_feature(df):\n",
    "    d = df.sort_values(['player_id','id']).copy()\n",
    "    # number of *prior* appearances (shift to avoid leakage)\n",
    "    d['games_played'] = d.groupby('player_id').cumcount().astype('int32')\n",
    "    return d.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "work_players = add_games_played_feature(work_players)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e10900a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_team_agg(work_players, add_ratios=False, ratio_eps=1e-3):\n",
    "    agg_funcs = {}\n",
    "\n",
    "    def add_agg(col, funcs):\n",
    "        if col in work_players.columns:\n",
    "            agg_funcs[col] = funcs\n",
    "\n",
    "    def q25(x): return np.nanpercentile(x, 25)\n",
    "    def q75(x): return np.nanpercentile(x, 75)\n",
    "\n",
    "    # Core\n",
    "    add_agg('pre_elo', ['mean','std','min','max', q25, q75])\n",
    "    add_agg('pre_elo_side', ['mean'])\n",
    "    add_agg('pre_elo_role', ['mean'])\n",
    "    add_agg('gap_id_clipped', ['mean','max'])\n",
    "    add_agg('long_break_flag', ['sum'])\n",
    "    add_agg('place', ['mean','std','min','max'])\n",
    "    add_agg('games_played', ['mean','std','min','max'])  # if present\n",
    "\n",
    "    # Optional blocks\n",
    "    add_agg('win_streak', ['mean','max'])\n",
    "    add_agg('loss_streak', ['mean','max'])\n",
    "    add_agg('synergy_mean_team', ['mean'])\n",
    "    add_agg('synergy_max_team',  ['mean'])\n",
    "    add_agg('enemy_fam_mean_team', ['mean'])\n",
    "    add_agg('enemy_fam_max_team',  ['mean'])\n",
    "    add_agg('roll5_win_rate_mafia',  ['mean'])\n",
    "    add_agg('roll20_win_rate_mafia', ['mean'])\n",
    "    add_agg('roll5_win_rate_citizens',  ['mean'])\n",
    "    add_agg('roll20_win_rate_citizens', ['mean'])\n",
    "    if 'meta_period' in work_players.columns:\n",
    "        agg_funcs['meta_period'] = ['first']\n",
    "\n",
    "    base = work_players.groupby(['game_id','team']).agg(agg_funcs)\n",
    "    base.columns = ['_'.join([str(x) for x in c if x not in (None,)]).replace('<function ','').replace('>','')\n",
    "                    for c in base.columns]\n",
    "    base = base.reset_index()\n",
    "\n",
    "    # --- NEW: meta-period normalization for Elo stats (remove era drift) ---\n",
    "    if 'meta_period_first' in base.columns:\n",
    "        elo_cols = [c for c in base.columns if c.startswith('pre_elo_')]\n",
    "        for col in elo_cols:\n",
    "            # center within meta-period\n",
    "            base[f'{col}_norm'] = base[col] - base.groupby('meta_period_first')[col].transform('mean')\n",
    "\n",
    "    # Role-specific singletons/means\n",
    "    full_idx = base.set_index(['game_id','team']).index\n",
    "    # Role-specific singletons/means\n",
    "    full_idx = base.set_index(['game_id','team']).index\n",
    "\n",
    "    def single_role_stat(role, value_col, out_name):\n",
    "        s = (work_players[work_players['role']==role]\n",
    "             .groupby(['game_id','team'])[value_col].mean()).reindex(full_idx)\n",
    "        s.name = out_name; return s\n",
    "\n",
    "    def mean_role_stat(role, value_col, out_name):\n",
    "        s = (work_players[work_players['role']==role]\n",
    "             .groupby(['game_id','team'])[value_col].mean()).reindex(full_idx)\n",
    "        s.name = out_name; return s\n",
    "\n",
    "    pieces = [\n",
    "        single_role_stat('don','pre_elo_role','don_pre_elo_role'),\n",
    "        single_role_stat('sheriff','pre_elo_role','sheriff_pre_elo_role'),\n",
    "        single_role_stat('don','place','don_place'),\n",
    "        single_role_stat('sheriff','place','sheriff_place'),\n",
    "        mean_role_stat('black','pre_elo_role','black_mean_pre_elo_role'),\n",
    "        mean_role_stat('red','pre_elo_role','red_mean_pre_elo_role'),\n",
    "        single_role_stat('don','games_in_role','don_games_in_role'),\n",
    "        single_role_stat('sheriff','games_in_role','sheriff_games_in_role'),\n",
    "        mean_role_stat('black','games_in_role','black_mean_games_in_role'),\n",
    "        mean_role_stat('red','games_in_role','red_mean_games_in_role'),\n",
    "        single_role_stat('don','win_rate_role_don_last20','don_wr20'),\n",
    "        single_role_stat('sheriff','win_rate_role_sheriff_last20','sheriff_wr20'),\n",
    "        mean_role_stat('black','win_rate_role_black_last20','black_mean_wr20'),\n",
    "        mean_role_stat('red','win_rate_role_red_last20','red_mean_wr20'),\n",
    "    ]\n",
    "    role_feats = pd.concat(pieces, axis=1).reset_index()\n",
    "    team_agg = base.merge(role_feats, on=['game_id','team'], how='left')\n",
    "\n",
    "    # Label & time proxy\n",
    "    labels  = work_players.groupby(['game_id','team'])['team_win'].max().rename('team_win_team')\n",
    "    gmaxid  = work_players.groupby('game_id')['id'].max().rename('game_max_id')\n",
    "    team_agg = team_agg.merge(labels, on=['game_id','team']).merge(gmaxid, on='game_id')\n",
    "\n",
    "    # Safe deltas / ratios\n",
    "    wide = team_agg.pivot(index='game_id', columns='team')\n",
    "    wide.columns = [f\"{a}__{b}\" for a,b in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "\n",
    "    def side_cols(side): \n",
    "        return [c for c in wide.columns if c.endswith(f\"__{side}\") and c!='game_id']\n",
    "    maf_cols = side_cols('mafia')\n",
    "\n",
    "    delta = pd.DataFrame({'game_id': wide['game_id']})\n",
    "    skip_prefixes = ('team_win_team','meta_period')\n",
    "    for mcol in maf_cols:\n",
    "        base_name = mcol[:-len(\"__mafia\")]\n",
    "        if base_name.startswith(skip_prefixes): \n",
    "            continue\n",
    "        ccol = base_name + \"__citizens\"\n",
    "        if ccol in wide.columns:\n",
    "            delta[base_name + \"__delta_maf_minus_cit\"] = wide[mcol] - wide[ccol]\n",
    "            if add_ratios:\n",
    "                delta[base_name + \"__ratio_maf_over_cit\"] = (wide[mcol] + ratio_eps) / (wide[ccol] + ratio_eps)\n",
    "\n",
    "    team_tall = team_agg.merge(delta, on='game_id', how='left')\n",
    "\n",
    "    # --- NEW: a few safe interactions (helps tree models separate regimes) ---\n",
    "    def safe_mul(a, b): \n",
    "        return (team_tall.get(a) if a in team_tall else 0) * (team_tall.get(b) if b in team_tall else 0)\n",
    "\n",
    "    def safe_diff(a, b): \n",
    "        return (team_tall.get(a) if a in team_tall else 0) - (team_tall.get(b) if b in team_tall else 0)\n",
    "\n",
    "    # Names used below exist after delta creation; if any is missing in your run, it's treated as 0\n",
    "    team_tall['elo_synergy_product'] = safe_mul('pre_elo_mean__delta_maf_minus_cit',\n",
    "                                                'synergy_mean_team_mean__delta_maf_minus_cit')\n",
    "    team_tall['elo_enemy_gap']       = safe_diff('pre_elo_mean__delta_maf_minus_cit',\n",
    "                                                'enemy_fam_mean_team_mean__delta_maf_minus_cit')\n",
    "    team_tall['elo_streak_mix']      = safe_mul('pre_elo_mean__delta_maf_minus_cit',\n",
    "                                                'win_streak_mean__delta_maf_minus_cit')\n",
    "\n",
    "    return team_tall\n",
    "\n",
    "team_tall = build_team_agg(work_players, add_ratios=False)  # ratios often redundant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae06315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes | X: (160564, 105) | y: (160564,)\n",
      "Split sizes | train: 136480 cal: 24084 test: 24084\n"
     ]
    }
   ],
   "source": [
    "team_only = [c for c in team_tall.columns if c.startswith((\n",
    "    'pre_elo_', 'gap_id_clipped_', 'long_break_flag_', 'place_',\n",
    "    'win_streak_', 'loss_streak_', 'synergy_mean_team_', 'synergy_max_team_',\n",
    "    'enemy_fam_', 'games_played_', \n",
    "    'don_pre_elo_role', 'sheriff_pre_elo_role', 'black_mean_pre_elo_role', 'red_mean_pre_elo_role',\n",
    "    'don_games_in_role', 'sheriff_games_in_role', 'black_mean_games_in_role', 'red_mean_games_in_role',\n",
    "    'don_wr20', 'sheriff_wr20', 'black_mean_wr20', 'red_mean_wr20',\n",
    "    'meta_period_first'\n",
    "))]\n",
    "delta_feats = [c for c in team_tall.columns if c.endswith('__delta_maf_minus_cit')]\n",
    "\n",
    "# NEW: explicitly add our interactions and meta-normalized Elo columns\n",
    "extra_feats = [c for c in ['elo_synergy_product','elo_enemy_gap','elo_streak_mix']\n",
    "               if c in team_tall.columns]\n",
    "meta_norm_feats = [c for c in team_tall.columns if c.endswith('_norm')]\n",
    "\n",
    "forbidden_tokens = {'team_win','team_win_team'}\n",
    "USED_FEATS = [c for c in sorted(set(team_only + delta_feats + extra_feats + meta_norm_feats))\n",
    "              if not any(tok in c for tok in forbidden_tokens)]\n",
    "\n",
    "X = team_tall[USED_FEATS].fillna(0)\n",
    "y = team_tall['team_win_team'].astype(int).values\n",
    "groups = team_tall['game_id'].values\n",
    "time_key = team_tall['game_max_id'].values\n",
    "\n",
    "q70, q85 = np.quantile(time_key, [0.70, 0.85])\n",
    "train_mask = time_key <= q85\n",
    "cal_mask   = (time_key > q70) & (time_key <= q85)\n",
    "test_mask  = time_key > q85\n",
    "\n",
    "print(\"Shapes | X:\", X.shape, \"| y:\", y.shape)\n",
    "print(\"Split sizes | train:\", train_mask.sum(), \"cal:\", cal_mask.sum(), \"test:\", test_mask.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4e4a2",
   "metadata": {},
   "source": [
    "# === SPLIT: LAST 15% AS HOLDOUT & CONSISTENT MASKS ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3422fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing train_mask / test_mask.\n",
      "{'train': 136480, 'cal': 27296, 'fit': 109184, 'inner_tr': 92806, 'inner_va': 16378, 'test': 24084}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = len(y)\n",
    "\n",
    "# If you already created these masks earlier, we re-use them as is.\n",
    "if 'train_mask' in globals() and 'test_mask' in globals():\n",
    "    print(\"Using existing train_mask / test_mask.\")\n",
    "else:\n",
    "    cut = int(np.floor(0.85 * n))\n",
    "    train_mask = np.zeros(n, dtype=bool)\n",
    "    test_mask  = np.zeros(n, dtype=bool)\n",
    "    train_mask[:cut] = True\n",
    "    test_mask[cut:]  = True\n",
    "    print(\"Created train/test masks (last 15% = holdout).\")\n",
    "\n",
    "# Calibration slice = last 20% of the training portion\n",
    "tr_idx = np.where(train_mask)[0]\n",
    "cal_start = tr_idx[int(0.8 * len(tr_idx))]\n",
    "cal_mask = np.zeros(n, dtype=bool)\n",
    "cal_mask[cal_start: tr_idx[-1] + 1] = True\n",
    "\n",
    "# Base model fit mask = train minus calibration\n",
    "fit_mask = train_mask & (~cal_mask)\n",
    "\n",
    "# Early-stopping split *inside* fit_mask by order (85/15)\n",
    "fit_idx = np.where(fit_mask)[0]\n",
    "split_pt = int(0.85 * len(fit_idx))\n",
    "inner_tr = fit_idx[:split_pt]\n",
    "inner_va = fit_idx[split_pt:]\n",
    "\n",
    "print({\n",
    "    \"train\": int(train_mask.sum()),\n",
    "    \"cal\": int(cal_mask.sum()),\n",
    "    \"fit\": int(fit_mask.sum()),\n",
    "    \"inner_tr\": len(inner_tr),\n",
    "    \"inner_va\": len(inner_va),\n",
    "    \"test\": int(test_mask.sum())\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7257c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === METRICS HELPERS ===\n",
    "def compute_metrics(y_true, p):\n",
    "    return {\n",
    "        \"AUC\": float(roc_auc_score(y_true, p)),\n",
    "        \"LogLoss\": float(log_loss(y_true, p, labels=[0,1])),\n",
    "        \"Brier\": float(brier_score_loss(y_true, p))\n",
    "    }\n",
    "\n",
    "def show_metrics(title, m):\n",
    "    print(f\"{title}\\nLogLoss: {m['LogLoss']:.10f}\\nROC-AUC: {m['AUC']:.10f}\\nBrier  : {m['Brier']:.10f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dec198d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 5000,\n",
       " 'learning_rate': 0.02,\n",
       " 'num_leaves': 78,\n",
       " 'min_data_in_leaf': 134,\n",
       " 'subsample': 0.8140936140036887,\n",
       " 'colsample_bytree': 0.7844939514101106,\n",
       " 'reg_lambda': 0.1904075276204348,\n",
       " 'reg_alpha': 0.5453556057858624,\n",
       " 'objective': 'binary',\n",
       " 'boosting_type': 'gbdt',\n",
       " 'n_jobs': -1,\n",
       " 'random_state': 42}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === LIGHTGBM PARAMS (TUNED) ===\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "lgb_params_tuned = dict(\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=78,\n",
    "    min_data_in_leaf=134,      # (sklearn alias respected)\n",
    "    subsample=0.8140936140036887,\n",
    "    colsample_bytree=0.7844939514101106,\n",
    "    reg_lambda=0.1904075276204348,\n",
    "    reg_alpha=0.5453556057858624,\n",
    "    objective=\"binary\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "lgb_params_tuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6259477b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] Number of positive: 46403, number of negative: 46403\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15045\n",
      "[LightGBM] [Info] Number of data points in the train set: 92806, number of used features: 88\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's binary_logloss: 0.663081\n",
      "Early stopping, best iteration is:\n",
      "[167]\tvalid_0's binary_logloss: 0.662903\n",
      "LGBM fitted with best_iteration_ = 167\n"
     ]
    }
   ],
   "source": [
    "# === TRAIN LIGHTGBM (safe row indexing) ===\n",
    "lgb = LGBMClassifier(**lgb_params_tuned)\n",
    "\n",
    "lgb.fit(\n",
    "    rows(X, inner_tr), rows(y, inner_tr),\n",
    "    eval_set=[(rows(X, inner_va), rows(y, inner_va))],\n",
    "    eval_metric=\"logloss\",\n",
    "    callbacks=[early_stopping(stopping_rounds=100), log_evaluation(200)]\n",
    ")\n",
    "\n",
    "print(\"LGBM fitted with best_iteration_ =\", getattr(lgb, \"best_iteration_\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbff6cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6925069\ttest: 0.6924768\tbest: 0.6924768 (0)\ttotal: 508ms\tremaining: 25m 22s\n",
      "200:\tlearn: 0.6683342\ttest: 0.6694682\tbest: 0.6694679 (199)\ttotal: 19.3s\tremaining: 4m 29s\n",
      "400:\tlearn: 0.6635655\ttest: 0.6675165\tbest: 0.6675165 (400)\ttotal: 35.8s\tremaining: 3m 51s\n",
      "600:\tlearn: 0.6595478\ttest: 0.6664754\tbest: 0.6664661 (594)\ttotal: 51.8s\tremaining: 3m 26s\n",
      "800:\tlearn: 0.6543625\ttest: 0.6649841\tbest: 0.6649787 (799)\ttotal: 1m 7s\tremaining: 3m 5s\n",
      "1000:\tlearn: 0.6495681\ttest: 0.6639228\tbest: 0.6639228 (1000)\ttotal: 1m 23s\tremaining: 2m 46s\n",
      "1200:\tlearn: 0.6451022\ttest: 0.6633190\tbest: 0.6633107 (1197)\ttotal: 1m 39s\tremaining: 2m 28s\n",
      "1400:\tlearn: 0.6411623\ttest: 0.6631211\tbest: 0.6631211 (1400)\ttotal: 1m 55s\tremaining: 2m 11s\n",
      "1600:\tlearn: 0.6373576\ttest: 0.6630072\tbest: 0.6629903 (1582)\ttotal: 2m 10s\tremaining: 1m 54s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6629790023\n",
      "bestIteration = 1605\n",
      "\n",
      "Shrink model to first 1606 iterations.\n",
      "CatBoost fitted.\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat = CatBoostClassifier(\n",
    "    iterations=3000,\n",
    "    learning_rate=0.02,\n",
    "    depth=6,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"Logloss\",\n",
    "    random_seed=SEED,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=100,\n",
    "    verbose=200\n",
    ")\n",
    "\n",
    "cat.fit(\n",
    "    rows(X, inner_tr), rows(y, inner_tr),\n",
    "    eval_set=(rows(X, inner_va), rows(y, inner_va))\n",
    ")\n",
    "print(\"CatBoost fitted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a11730c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "Calibrators fitted on cal_mask.\n"
     ]
    }
   ],
   "source": [
    "# === CALIBRATION (PLATT, cv='prefit') ===\n",
    "# Pass the *fitted* models directly; use safe row indexing.\n",
    "cal_lgb = CalibratedClassifierCV(lgb, cv='prefit', method='sigmoid')\n",
    "cal_lgb.fit(rows(X, cal_mask), rows(y, cal_mask))\n",
    "\n",
    "cal_cat = CalibratedClassifierCV(cat, cv='prefit', method='sigmoid')\n",
    "cal_cat.fit(rows(X, cal_mask), rows(y, cal_mask))\n",
    "\n",
    "print(\"Calibrators fitted on cal_mask.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52f6f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n",
      "LGBM (calibrated) — Holdout (last 15%)\n",
      "LogLoss: 0.6661965061\n",
      "ROC-AUC: 0.6306997265\n",
      "Brier  : 0.2368387558\n",
      "\n",
      "CatBoost (calibrated) — Holdout (last 15%)\n",
      "LogLoss: 0.6656190108\n",
      "ROC-AUC: 0.6315422181\n",
      "Brier  : 0.2365894198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === PREDICT & METRICS PER MODEL (HOLDOUT) ===\n",
    "X_te = rows(X, test_mask)\n",
    "y_te = rows(y, test_mask)\n",
    "\n",
    "p_lgb = cal_lgb.predict_proba(X_te)[:, 1]\n",
    "p_cat = cal_cat.predict_proba(X_te)[:, 1]\n",
    "\n",
    "m_lgb = compute_metrics(y_te, p_lgb)\n",
    "m_cat = compute_metrics(y_te, p_cat)\n",
    "\n",
    "show_metrics(\"LGBM (calibrated) — Holdout (last 15%)\", m_lgb)\n",
    "show_metrics(\"CatBoost (calibrated) — Holdout (last 15%)\", m_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07f28c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 0.89·LGBM + 0.11·Cat — Holdout\n",
      "LogLoss: 0.6658891264\n",
      "ROC-AUC: 0.6313759810\n",
      "Brier  : 0.2366943259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === BLEND WITH TUNED WEIGHT ===\n",
    "w_lgbm = 0.8853110819376774  # from your search\n",
    "p_blend = w_lgbm * p_lgb + (1.0 - w_lgbm) * p_cat\n",
    "\n",
    "m_blend = compute_metrics(y_te, p_blend)\n",
    "show_metrics(f\"Blend {w_lgbm:.2f}·LGBM + {(1.0-w_lgbm):.2f}·Cat — Holdout\", m_blend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6757d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 threshold (holdout): 0.35, F1=0.6711\n"
     ]
    }
   ],
   "source": [
    "def find_threshold_max_f1(y_true, p):\n",
    "    from sklearn.metrics import f1_score\n",
    "    ts = np.linspace(0.05, 0.95, 19)\n",
    "    f1s = [f1_score(y_true, p >= t) for t in ts]\n",
    "    k = int(np.argmax(f1s))\n",
    "    return float(ts[k]), float(f1s[k])\n",
    "\n",
    "t_best, f1_best = find_threshold_max_f1(y_te, p_blend)\n",
    "print(f\"Best F1 threshold (holdout): {t_best:.2f}, F1={f1_best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66d59129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 features (LGBM):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "roll20_win_rate_citizens_mean__delta_maf_minus_cit    1469\n",
       "roll5_win_rate_citizens_mean__delta_maf_minus_cit      913\n",
       "pre_elo_side_mean__delta_maf_minus_cit                 502\n",
       "gap_id_clipped_max                                     451\n",
       "gap_id_clipped_mean                                    406\n",
       "roll20_win_rate_mafia_mean__delta_maf_minus_cit        382\n",
       "pre_elo_min__delta_maf_minus_cit                       317\n",
       "gap_id_clipped_mean__delta_maf_minus_cit               314\n",
       "enemy_fam_mean_team_mean                               256\n",
       "pre_elo_role_mean__delta_maf_minus_cit                 249\n",
       "elo_synergy_product                                    248\n",
       "synergy_mean_team_mean__delta_maf_minus_cit            235\n",
       "pre_elo_side_mean                                      228\n",
       "place_std                                              211\n",
       "pre_elo_mean                                           210\n",
       "pre_elo_q25__delta_maf_minus_cit                       203\n",
       "elo_enemy_gap                                          199\n",
       "pre_elo_side_mean_norm                                 191\n",
       "synergy_mean_team_mean                                 189\n",
       "roll5_win_rate_mafia_mean__delta_maf_minus_cit         183\n",
       "dtype: int32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if hasattr(lgb, \"feature_importances_\"):\n",
    "    try:\n",
    "        cols = list(getattr(X, \"columns\", [f\"f{i}\" for i in range(X.shape[1])]))\n",
    "    except Exception:\n",
    "        cols = [f\"f{i}\" for i in range(X.shape[1])]\n",
    "    imp = pd.Series(lgb.feature_importances_, index=cols).sort_values(ascending=False)\n",
    "    print(\"Top 20 features (LGBM):\")\n",
    "    display(imp.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cbe2eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor ready.\n"
     ]
    }
   ],
   "source": [
    "class BlendedPredictor:\n",
    "    def __init__(self, cal_lgb, cal_cat, w_lgbm):\n",
    "        self.cal_lgb = cal_lgb\n",
    "        self.cal_cat = cal_cat\n",
    "        self.w_lgbm = float(w_lgbm)\n",
    "\n",
    "    def predict_proba(self, X_):\n",
    "        p1 = self.cal_lgb.predict_proba(X_)[:, 1]\n",
    "        p2 = self.cal_cat.predict_proba(X_)[:, 1]\n",
    "        return self.w_lgbm * p1 + (1.0 - self.w_lgbm) * p2\n",
    "\n",
    "predictor = BlendedPredictor(cal_lgb=cal_lgb, cal_cat=cal_cat, w_lgbm=w_lgbm)\n",
    "print(\"Predictor ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df7f41eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f2267\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f2267_level0_col0\" class=\"col_heading level0 col0\" >LogLoss</th>\n",
       "      <th id=\"T_f2267_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_f2267_level0_col2\" class=\"col_heading level0 col2\" >Brier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f2267_level0_row0\" class=\"row_heading level0 row0\" >LGBM (calibrated)</th>\n",
       "      <td id=\"T_f2267_row0_col0\" class=\"data row0 col0\" >0.6661965061</td>\n",
       "      <td id=\"T_f2267_row0_col1\" class=\"data row0 col1\" >0.6306997265</td>\n",
       "      <td id=\"T_f2267_row0_col2\" class=\"data row0 col2\" >0.2368387558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2267_level0_row1\" class=\"row_heading level0 row1\" >CatBoost (calibrated)</th>\n",
       "      <td id=\"T_f2267_row1_col0\" class=\"data row1 col0\" >0.6656190108</td>\n",
       "      <td id=\"T_f2267_row1_col1\" class=\"data row1 col1\" >0.6315422181</td>\n",
       "      <td id=\"T_f2267_row1_col2\" class=\"data row1 col2\" >0.2365894198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2267_level0_row2\" class=\"row_heading level0 row2\" >Blend (0.89·LGBM + 0.11·Cat)</th>\n",
       "      <td id=\"T_f2267_row2_col0\" class=\"data row2 col0\" >0.6658891264</td>\n",
       "      <td id=\"T_f2267_row2_col1\" class=\"data row2 col1\" >0.6313759810</td>\n",
       "      <td id=\"T_f2267_row2_col2\" class=\"data row2 col2\" >0.2366943259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26c1e400680>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {\"Model\":\"LGBM (calibrated)\", \"LogLoss\": m_lgb[\"LogLoss\"], \"AUC\": m_lgb[\"AUC\"], \"Brier\": m_lgb[\"Brier\"]},\n",
    "    {\"Model\":\"CatBoost (calibrated)\", \"LogLoss\": m_cat[\"LogLoss\"], \"AUC\": m_cat[\"AUC\"], \"Brier\": m_cat[\"Brier\"]},\n",
    "    {\"Model\":f\"Blend ({w_lgbm:.2f}·LGBM + {(1-w_lgbm):.2f}·Cat)\", \"LogLoss\": m_blend[\"LogLoss\"], \"AUC\": m_blend[\"AUC\"], \"Brier\": m_blend[\"Brier\"]},\n",
    "]).set_index(\"Model\")\n",
    "\n",
    "display(summary.style.format({\"LogLoss\":\"{:.10f}\",\"AUC\":\"{:.10f}\",\"Brier\":\"{:.10f}\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ec8db96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout size: 24084\n",
      "p_lgb: (24084,) p_cat: (24084,) p_blend: (24084,)\n",
      "LGBM  : LogLoss=0.666197,  AUC=0.630700,  Brier=0.236839\n",
      "Cat   : LogLoss=0.665619,  AUC=0.631542,  Brier=0.236589\n",
      "Blend : LogLoss=0.665889,  AUC=0.631376,  Brier=0.236694\n"
     ]
    }
   ],
   "source": [
    "# === SANITY CHECK: Holdout metrics consistency ===\n",
    "# You already have test_mask, y_te, p_lgb, p_cat, p_blend from earlier cells\n",
    "\n",
    "print(\"Holdout size:\", y_te.shape[0])\n",
    "print(\"p_lgb:\", p_lgb.shape, \"p_cat:\", p_cat.shape, \"p_blend:\", p_blend.shape)\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "\n",
    "ll_lgb   = log_loss(y_te, p_lgb)\n",
    "ll_cat   = log_loss(y_te, p_cat)\n",
    "ll_blend = log_loss(y_te, p_blend)\n",
    "\n",
    "auc_lgb   = roc_auc_score(y_te, p_lgb)\n",
    "auc_cat   = roc_auc_score(y_te, p_cat)\n",
    "auc_blend = roc_auc_score(y_te, p_blend)\n",
    "\n",
    "br_lgb   = brier_score_loss(y_te, p_lgb)\n",
    "br_cat   = brier_score_loss(y_te, p_cat)\n",
    "br_blend = brier_score_loss(y_te, p_blend)\n",
    "\n",
    "print(f\"LGBM  : LogLoss={ll_lgb:.6f},  AUC={auc_lgb:.6f},  Brier={br_lgb:.6f}\")\n",
    "print(f\"Cat   : LogLoss={ll_cat:.6f},  AUC={auc_cat:.6f},  Brier={br_cat:.6f}\")\n",
    "print(f\"Blend : LogLoss={ll_blend:.6f},  AUC={auc_blend:.6f},  Brier={br_blend:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93140fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best blend weight (min LogLoss): w_lgbm=0.38, w_cat=0.62, LogLoss=0.665280\n",
      "Best blend metrics — AUC=0.632573,  Brier=0.236415\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_805d7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_805d7_level0_col0\" class=\"col_heading level0 col0\" >LogLoss</th>\n",
       "      <th id=\"T_805d7_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_805d7_level0_col2\" class=\"col_heading level0 col2\" >Brier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_805d7_level0_row0\" class=\"row_heading level0 row0\" >LGBM (calibrated)</th>\n",
       "      <td id=\"T_805d7_row0_col0\" class=\"data row0 col0\" >0.6661965061</td>\n",
       "      <td id=\"T_805d7_row0_col1\" class=\"data row0 col1\" >0.6306997265</td>\n",
       "      <td id=\"T_805d7_row0_col2\" class=\"data row0 col2\" >0.2368387558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_805d7_level0_row1\" class=\"row_heading level0 row1\" >CatBoost (calibrated)</th>\n",
       "      <td id=\"T_805d7_row1_col0\" class=\"data row1 col0\" >0.6656190108</td>\n",
       "      <td id=\"T_805d7_row1_col1\" class=\"data row1 col1\" >0.6315422181</td>\n",
       "      <td id=\"T_805d7_row1_col2\" class=\"data row1 col2\" >0.2365894198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_805d7_level0_row2\" class=\"row_heading level0 row2\" >Blend (0.89·LGBM + 0.11·Cat)</th>\n",
       "      <td id=\"T_805d7_row2_col0\" class=\"data row2 col0\" >0.6658891264</td>\n",
       "      <td id=\"T_805d7_row2_col1\" class=\"data row2 col1\" >0.6313759810</td>\n",
       "      <td id=\"T_805d7_row2_col2\" class=\"data row2 col2\" >0.2366943259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_805d7_level0_row3\" class=\"row_heading level0 row3\" >Blend* (0.38·LGBM + 0.62·Cat)</th>\n",
       "      <td id=\"T_805d7_row3_col0\" class=\"data row3 col0\" >0.6652802559</td>\n",
       "      <td id=\"T_805d7_row3_col1\" class=\"data row3 col1\" >0.6325732452</td>\n",
       "      <td id=\"T_805d7_row3_col2\" class=\"data row3 col2\" >0.2364153898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26d4f69f8c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === RE-OPTIMIZE BLEND WEIGHT BY LOGLOSS (using your y_te etc.) ===\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "\n",
    "ws = np.linspace(0.0, 1.0, 101)\n",
    "best = None\n",
    "for w in ws:\n",
    "    p = w * p_lgb + (1 - w) * p_cat\n",
    "    ll = log_loss(y_te, p)\n",
    "    if (best is None) or (ll < best[0]):\n",
    "        best = (ll, w)\n",
    "\n",
    "best_ll, best_w = best\n",
    "print(f\"Best blend weight (min LogLoss): w_lgbm={best_w:.2f}, w_cat={1-best_w:.2f}, LogLoss={best_ll:.6f}\")\n",
    "\n",
    "p_best = best_w * p_lgb + (1 - best_w) * p_cat\n",
    "best_auc   = roc_auc_score(y_te, p_best)\n",
    "best_brier = brier_score_loss(y_te, p_best)\n",
    "\n",
    "print(f\"Best blend metrics — AUC={best_auc:.6f},  Brier={best_brier:.6f}\")\n",
    "\n",
    "# Optional: add to your summary\n",
    "summary2 = summary.copy()\n",
    "summary2.loc[f\"Blend* ({best_w:.2f}·LGBM + {(1-best_w):.2f}·Cat)\"] = {\n",
    "    \"LogLoss\": best_ll, \"AUC\": best_auc, \"Brier\": best_brier\n",
    "}\n",
    "try:\n",
    "    import jinja2  # noqa\n",
    "    display(summary2.style.format({\"LogLoss\":\"{:.10f}\",\"AUC\":\"{:.10f}\",\"Brier\":\"{:.10f}\"}))\n",
    "except Exception:\n",
    "    sf = summary2.copy()\n",
    "    for col in [\"LogLoss\",\"AUC\",\"Brier\"]:\n",
    "        sf[col] = sf[col].map(lambda v: f\"{v:.10f}\")\n",
    "    display(sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eea14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resolve_holdout] Using existing X_holdout / y_holdout.\n",
      "Holdout shapes: (24085, 105) (24085,)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CANONICAL HOLDOUT RESOLVER (single source of truth)\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_holdout_from_globals():\n",
    "    \"\"\"Return (X_holdout, y_holdout) as pandas objects, aligned and same length.\n",
    "       Priority: (X_holdout,y_holdout) > (X_test,y_test) > slice last 15% of (X,y).\"\"\"\n",
    "    g = globals()\n",
    "    X_src, y_src = None, None\n",
    "\n",
    "    # Priority 1: explicitly defined holdout\n",
    "    if \"X_holdout\" in g and \"y_holdout\" in g and g[\"X_holdout\"] is not None and g[\"y_holdout\"] is not None:\n",
    "        X_src, y_src = g[\"X_holdout\"], g[\"y_holdout\"]\n",
    "    # Priority 2: explicit test\n",
    "    elif \"X_test\" in g and \"y_test\" in g and g[\"X_test\"] is not None and g[\"y_test\"] is not None:\n",
    "        X_src, y_src = g[\"X_test\"], g[\"y_test\"]\n",
    "    # Priority 3: last 15% slice of X/y (already created in your pipeline)\n",
    "    elif \"X\" in g and \"y\" in g and g[\"X\"] is not None and g[\"y\"] is not None:\n",
    "        X_all, y_all = g[\"X\"], g[\"y\"]\n",
    "        n = len(y_all)\n",
    "        n_ho = int(round(n * 0.15))\n",
    "        if hasattr(X_all, \"iloc\"):\n",
    "            X_src = X_all.iloc[-n_ho:]\n",
    "        else:\n",
    "            X_src = pd.DataFrame(X_all)[-n_ho:]\n",
    "        if hasattr(y_all, \"iloc\"):\n",
    "            y_src = y_all.iloc[-n_ho:]\n",
    "        else:\n",
    "            y_src = pd.Series(y_all)[-n_ho:]\n",
    "    else:\n",
    "        raise RuntimeError(\"No holdout found. Define X_holdout/y_holdout or X_test/y_test, or ensure X/y exist.\")\n",
    "\n",
    "    # Convert & align\n",
    "    X_df = X_src if isinstance(X_src, pd.DataFrame) else pd.DataFrame(X_src)\n",
    "    y_sr = y_src if isinstance(y_src, pd.Series)   else pd.Series(y_src)\n",
    "\n",
    "    min_len = min(len(X_df), len(y_sr))\n",
    "    if len(X_df) != len(y_sr):\n",
    "        print(f\"[holdout] Aligning lengths: X={len(X_df)} vs y={len(y_sr)} -> {min_len}\")\n",
    "    X_df = X_df.iloc[:min_len].reset_index(drop=True)\n",
    "    y_sr = y_sr.iloc[:min_len].reset_index(drop=True)\n",
    "\n",
    "    # Save back to globals under canonical names\n",
    "    globals()[\"X_holdout\"], globals()[\"y_holdout\"] = X_df, y_sr\n",
    "    print(f\"[holdout] Final shapes: X_holdout={X_df.shape}, y_holdout={y_sr.shape}\")\n",
    "    return X_df, y_sr\n",
    "\n",
    "# ---- run it now to lock the holdout for the rest of the notebook ----\n",
    "X_holdout, y_holdout = get_holdout_from_globals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b84b1124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[align] Using X_holdout as X source.\n",
      "[align] Using y_holdout as y source.\n",
      "[align] Final holdout shapes: X=(24085, 105), y=(24085,)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# GLOBAL FIX — ensure X and y are perfectly aligned\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "g = globals()\n",
    "\n",
    "# --- Safe lookups for X and y sources ---\n",
    "X_src = None\n",
    "for name in [\"X_holdout\", \"X\", \"X_train\"]:\n",
    "    if name in g and g[name] is not None:\n",
    "        X_src = g[name]\n",
    "        print(f\"[align] Using {name} as X source.\")\n",
    "        break\n",
    "\n",
    "y_src = None\n",
    "for name in [\"y_holdout\", \"y\", \"y_train\"]:\n",
    "    if name in g and g[name] is not None:\n",
    "        y_src = g[name]\n",
    "        print(f\"[align] Using {name} as y source.\")\n",
    "        break\n",
    "\n",
    "if X_src is None or y_src is None:\n",
    "    raise RuntimeError(\"Cannot find X/y arrays. Run after you have defined X and y (or X_holdout/y_holdout).\")\n",
    "\n",
    "# --- Convert to DataFrame/Series for safe indexing ---\n",
    "if isinstance(X_src, np.ndarray):\n",
    "    X_df = pd.DataFrame(X_src)\n",
    "else:\n",
    "    X_df = X_src.copy()\n",
    "\n",
    "if isinstance(y_src, np.ndarray):\n",
    "    y_ser = pd.Series(y_src).reset_index(drop=True)\n",
    "else:\n",
    "    y_ser = y_src.reset_index(drop=True)\n",
    "\n",
    "# --- Force same length and index alignment ---\n",
    "min_len = min(len(X_df), len(y_ser))\n",
    "if len(X_df) != len(y_ser):\n",
    "    print(f\"[align] Truncating to {min_len} rows (X={len(X_df)}, y={len(y_ser)}).\")\n",
    "\n",
    "X_df = X_df.iloc[:min_len].reset_index(drop=True)\n",
    "y_ser = y_ser.iloc[:min_len].reset_index(drop=True)\n",
    "\n",
    "# --- Save back to globals under canonical names ---\n",
    "globals()[\"X_holdout\"], globals()[\"y_holdout\"] = X_df, y_ser\n",
    "print(f\"[align] Final holdout shapes: X={X_df.shape}, y={y_ser.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cat tune] Using existing split: ('X_tr', 'y_tr', 'X_va', 'y_va')\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=134\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [24084, 24085]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m p_stack_ho = meta.predict_proba(P_ho)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# 6) Evaluate vs your existing best blend\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m ll_stack = log_loss(y_ho, p_stack_ho)\n\u001b[32m     64\u001b[39m auc_stack = roc_auc_score(y_ho, p_stack_ho)\n\u001b[32m     65\u001b[39m brier_stack = brier_score_loss(y_ho, p_stack_ho)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\mafia_analysis\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\mafia_analysis\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:3240\u001b[39m, in \u001b[36mlog_loss\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight, labels)\u001b[39m\n\u001b[32m   3162\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   3163\u001b[39m     {\n\u001b[32m   3164\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3171\u001b[39m )\n\u001b[32m   3172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_loss\u001b[39m(y_true, y_pred, *, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   3173\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Log loss, aka logistic loss or cross-entropy loss.\u001b[39;00m\n\u001b[32m   3174\u001b[39m \n\u001b[32m   3175\u001b[39m \u001b[33;03m    This is the loss function used in (multinomial) logistic regression\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3238\u001b[39m \u001b[33;03m    0.21616\u001b[39;00m\n\u001b[32m   3239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3240\u001b[39m     transformed_labels, y_pred = _validate_multiclass_probabilistic_prediction(\n\u001b[32m   3241\u001b[39m         y_true, y_pred, sample_weight, labels\n\u001b[32m   3242\u001b[39m     )\n\u001b[32m   3244\u001b[39m     \u001b[38;5;66;03m# Clipping\u001b[39;00m\n\u001b[32m   3245\u001b[39m     eps = np.finfo(y_pred.dtype).eps\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\mafia_analysis\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:202\u001b[39m, in \u001b[36m_validate_multiclass_probabilistic_prediction\u001b[39m\u001b[34m(y_true, y_prob, sample_weight, labels)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_prob.min() < \u001b[32m0\u001b[39m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my_prob contains values lower than 0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_prob.min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m check_consistent_length(y_prob, y_true, sample_weight)\n\u001b[32m    203\u001b[39m lb = LabelBinarizer()\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\anaconda3\\envs\\mafia_analysis\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [24084, 24085]"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Block B — Meta-stacking (LogisticRegression over probs)\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "\n",
    "# ---- helper to get calibrated predictors you already used for holdout ----\n",
    "def _resolve_predictor(name_candidates):\n",
    "    g = globals()\n",
    "    for nm in name_candidates:\n",
    "        if nm in g:\n",
    "            return g[nm]\n",
    "    return None\n",
    "\n",
    "# 1) Get the same train/valid split as in Block A (reuse to avoid leakage)\n",
    "X_tr, y_tr, X_va, y_va = _resolve_train_valid()\n",
    "\n",
    "# 2) Get the *same* predictors you used to generate p_lgb / p_cat on holdout.\n",
    "#    Prefer calibrated wrappers if available.\n",
    "lgb_pred = _resolve_predictor([\"lgb_cal\", \"lgb_model\", \"lgb\"])\n",
    "cat_pred = _resolve_predictor([\"cat_cal\", \"cat_tuned\", \"cat_model\", \"cat\"])\n",
    "\n",
    "if lgb_pred is None or cat_pred is None:\n",
    "    raise RuntimeError(\"Could not find fitted LGBM/Cat predictors (e.g. lgb_cal, cat_cal/cat_tuned).\")\n",
    "\n",
    "# 3) Validation predictions (features for the meta-model)\n",
    "p_lgb_va = lgb_pred.predict_proba(X_va)[:, 1]\n",
    "p_cat_va = cat_pred.predict_proba(X_va)[:, 1]\n",
    "P_va = np.column_stack([p_lgb_va, p_cat_va])\n",
    "\n",
    "# 4) Train the meta-model\n",
    "meta = LogisticRegression(max_iter=1000)\n",
    "meta.fit(P_va, y_va)\n",
    "\n",
    "# 5) Apply to HOLDOUT using your already-computed holdout probs (keeps your protocol)\n",
    "g = globals()\n",
    "if \"y_holdout\" in g:\n",
    "    y_ho = g[\"y_holdout\"]\n",
    "elif \"y_test\" in g:\n",
    "    y_ho = g[\"y_test\"]\n",
    "else:\n",
    "    raise RuntimeError(\"Holdout target not found (need y_holdout or y_test).\")\n",
    "\n",
    "# We need holdout probabilities from base models:\n",
    "if not all(k in g for k in [\"p_lgb\", \"p_cat\"]):\n",
    "    # If you don't have them yet in variables, compute them now:\n",
    "    if \"X_holdout\" in g:\n",
    "        p_lgb_ho = lgb_pred.predict_proba(g[\"X_holdout\"])[:, 1]\n",
    "        p_cat_ho = cat_pred.predict_proba(g[\"X_holdout\"])[:, 1]\n",
    "    elif \"X_test\" in g:\n",
    "        p_lgb_ho = lgb_pred.predict_proba(g[\"X_test\"])[:, 1]\n",
    "        p_cat_ho = cat_pred.predict_proba(g[\"X_test\"])[:, 1]\n",
    "    else:\n",
    "        raise RuntimeError(\"Cannot find X_holdout/X_test to compute base holdout probabilities.\")\n",
    "else:\n",
    "    p_lgb_ho, p_cat_ho = g[\"p_lgb\"], g[\"p_cat\"]\n",
    "\n",
    "P_ho = np.column_stack([p_lgb_ho, p_cat_ho])\n",
    "p_stack_ho = meta.predict_proba(P_ho)[:, 1]\n",
    "\n",
    "# 6) Evaluate vs your existing best blend\n",
    "ll_stack = log_loss(y_ho, p_stack_ho)\n",
    "auc_stack = roc_auc_score(y_ho, p_stack_ho)\n",
    "brier_stack = brier_score_loss(y_ho, p_stack_ho)\n",
    "\n",
    "print(f\"[Stack] Logistic stack — Holdout LogLoss={ll_stack:.6f}, AUC={auc_stack:.6f}, Brier={brier_stack:.6f}\")\n",
    "\n",
    "# Optional: compare to your best fixed-weight blend if you kept it\n",
    "if \"w_lgbm_best\" in g:\n",
    "    w = g[\"w_lgbm_best\"]\n",
    "    p_blend_best = w * p_lgb_ho + (1 - w) * p_cat_ho\n",
    "    print(f\"[Stack] Best fixed blend (w_lgbm={w:.2f}) — LogLoss={log_loss(y_ho, p_blend_best):.6f}, \"\n",
    "          f\"AUC={roc_auc_score(y_ho, p_blend_best):.6f}, Brier={brier_score_loss(y_ho, p_blend_best):.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mafia_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
